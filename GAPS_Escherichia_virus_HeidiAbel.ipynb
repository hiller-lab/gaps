{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c4e791-43ef-4d0b-8051-ea88581d0bd6",
   "metadata": {},
   "source": [
    "# GAPS: Gene Annotation Pipeline from sequence and structure Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b08654c-9097-4f48-93be-7590a16031af",
   "metadata": {},
   "source": [
    "## 1. Initialize GAPS\n",
    "\n",
    "### 1.1. External software and databases\n",
    "\n",
    "The pipeline requires external software and databases to be installed. Read the <a href=\"./readme.md\">readme.md</a> file for further information.\n",
    "\n",
    "### 1.2. Python dependencies\n",
    "\n",
    "The pipeline requires few libraries and functions to run. These are defined in the (collapsed) code block below. Therefore, it must be executed before running the other code blocks.\n",
    "\n",
    "Don't forget to set the kernel to use the `gaps_env` environment before starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b633ef8-05e0-48e2-b460-53e257d20974",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run this code block first\n",
    "\n",
    "import os\n",
    "import string\n",
    "import shutil\n",
    "import random\n",
    "import subprocess\n",
    "import time\n",
    "import builtins\n",
    "import pickle\n",
    "from textwrap import wrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import csv\n",
    "import json\n",
    "from Bio import SeqIO, PDB\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Replace standard functions\n",
    "\n",
    "def ctime(e=\"%d-%m-%Y, %H:%M:%S\"):\n",
    "    t = time.localtime()\n",
    "    current_time = time.strftime(e, t)\n",
    "    return current_time\n",
    "\n",
    "def print(*objs, **kwargs):\n",
    "    prefix = f\"[{ctime()}]\"\n",
    "    builtins.print(prefix, *objs, **kwargs)\n",
    "\n",
    "def mdir(dir, chmod=None, verbose=True, wipe_first=False):\n",
    "    # If dir exists, delete it and its contents\n",
    "    if os.path.exists(dir) and wipe_first is True: \n",
    "        shutil.rmtree(dir)\n",
    "        if verbose: print(f\"Folder '{dir}' and its contents were deleted.\")\n",
    "    \n",
    "    # Create a new dir\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        if verbose: print(f\"Folder '{dir}' was created.\")\n",
    "\n",
    "    if chmod != None and int(chmod) != int(oct(os.stat(dir).st_mode)[-3:]):\n",
    "        octal_chmod = int(str(chmod), 8)\n",
    "        os.chmod(dir, octal_chmod)\n",
    "        if verbose: print(f\"Permissions of '{dir}' were changed to: {chmod}\")\n",
    "\n",
    "    return dir\n",
    "\n",
    "    \n",
    "# Import sequences\n",
    "\n",
    "def fix_ids(id_list):\n",
    "    new_id_list = []\n",
    "    padding = len(str(len(id_list)))\n",
    "    id_counter = 1\n",
    "    for id in id_list:\n",
    "        # Remove funny characters and limit to 40 characters\n",
    "        valid_characters = \"-_.() %s%s\" % (string.ascii_letters, string.digits)\n",
    "        sanitized_id = ''.join(c for c in id if c in valid_characters)[:40]\n",
    "        # Add incremental prefix if missing\n",
    "        prefix = f\"{id_counter:0{padding}d}_\"\n",
    "        if sanitized_id.startswith(prefix) == False:\n",
    "            sanitized_id = f\"{prefix}{sanitized_id}\"\n",
    "            \n",
    "        id_counter += 1\n",
    "        new_id_list.append(sanitized_id)\n",
    "    return new_id_list\n",
    "\n",
    "def import_sequences(file_path):\n",
    "    \n",
    "    id_list = []\n",
    "    loc_list = []\n",
    "    seq_list = []\n",
    "    imported_annotations = []\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "    \n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        if file_extension == '.fasta':\n",
    "            # Process FASTA file\n",
    "            with open(file_path, 'r') as fasta_file:\n",
    "                for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "                    id_list.append(record.id)\n",
    "                    seq_list.append(str(record.seq).upper())\n",
    "                    imported_annotations.append(record.description.lstrip(record.id).strip())\n",
    "    \n",
    "        elif file_extension == '.xlsx':\n",
    "            # Process Excel file\n",
    "            data = pd.read_excel(file_path, header=None)\n",
    "            id_list = data[0].tolist()\n",
    "            seq_list = data[1].tolist()\n",
    "            seq_list = [s.upper() for s in seq_list]\n",
    "            try:\n",
    "                imported_annotations = data[2].tolist()\n",
    "            except:\n",
    "                imported_annotations = []\n",
    "    \n",
    "        elif file_extension == '.csv':\n",
    "            # Process CSV file\n",
    "            data = pd.read_csv(file_path, header=None)\n",
    "            id_list = data[0].tolist()\n",
    "            seq_list = data[1].tolist()\n",
    "            seq_list = [s.upper() for s in seq_list]\n",
    "            try:\n",
    "                imported_annotations = data[2].tolist()\n",
    "            except:\n",
    "                imported_annotations = []\n",
    "    \n",
    "        elif file_extension == '.gb' or file_extension == '.gbk':\n",
    "            # Process GenBank file\n",
    "            with open(file_path, 'r') as gb_file:\n",
    "                for record in SeqIO.parse(gb_file, \"genbank\"):\n",
    "                    \"\"\"\n",
    "                    Properties that are obtained from a genbank file:\n",
    "                    record.id\n",
    "                    record.seq\n",
    "                    record.name\n",
    "                    record.description\n",
    "                    record.features\n",
    "                    record.annotations\n",
    "                    \"\"\"\n",
    "                    count = 1\n",
    "                    for feature in record.features:\n",
    "                        if feature.type == 'CDS':\n",
    "                            \n",
    "                            cds = feature.extract(record) # Extracts CDS region from input genome (=record) as a SeqRecord object\n",
    "\n",
    "                            imported_annotations.append(cds.description)\n",
    "                            \n",
    "                            dnaseq = str(cds.seq)\n",
    "                            \n",
    "                            if dnaseq.startswith(\"GTG\") or dnaseq.startswith(\"CTG\") or dnaseq.startswith(\"TTG\"):\n",
    "                                # Some times, GTG/CTG/TTG instead of ATG are used as a start codon.\n",
    "                                dnaseq = \"A\" + dnaseq[1:]\n",
    "                            elif dnaseq.startswith(\"ATG\") is False:\n",
    "                                print(\"No start codon found:\", str(feature.qualifiers['ID']))\n",
    "                                dnaseq = \"ATG\" + dnaseq\n",
    "                                \n",
    "                            try:\n",
    "                                aaseq = str(Seq(dnaseq).translate()).replace(\"*\", \"\").strip()\n",
    "                                seq_list.append(aaseq)                               \n",
    "                                #id_list.append(str(feature.qualifiers['label'])[2:-2].strip().replace(\" \", \"_\"))\n",
    "                                id_list.append(str(feature.qualifiers['ID']))\n",
    "                                loc_list.append(str(feature.location).replace(\"join{\", \"\").replace(\"}\", \"\"))\n",
    "                            except:\n",
    "                                #print(f\"Sequence {str(feature.qualifiers['label'])[2:-2].strip().replace(' ', '_')} could not be parsed!\")\n",
    "                                print(f\"Sequence {str(feature.qualifiers['ID'])} could not be parsed!\")\n",
    "\n",
    "                            count += 1\n",
    "    \n",
    "        else:\n",
    "            print(\"Unsupported file format.\")\n",
    "\n",
    "    else:\n",
    "        print(\"File does not exist.\")\n",
    "\n",
    "    # Sanitize the id names\n",
    "    short_unique_id_list = fix_ids(id_list)\n",
    "        \n",
    "    # Create db\n",
    "    db = {}\n",
    "    for i, unique_id in enumerate(short_unique_id_list):\n",
    "        db[unique_id] = {'name'                 : id_list[i].lstrip(\"['\").rstrip(\"']\"),\n",
    "                         'locus'                : loc_list[i] if len(loc_list) > 0 else \"\",\n",
    "                         'number'               : i,\n",
    "                         'sequence'             : seq_list[i],\n",
    "                         'sequence_length'      : len(seq_list[i]),\n",
    "                         'sequence_mw'          : \"%.2f\"%ProteinAnalysis(seq_list[i]).molecular_weight(),\n",
    "                         'sequence_file_path'   : \"\",\n",
    "                         'input_file_path'      : file_path,\n",
    "                         'imported_annotations' : imported_annotations[i],\n",
    "                         'domain_architecture'  : {'databases'         : {},\n",
    "                                                   'output_data'       : [],\n",
    "                                                   'output_file_paths' : [],\n",
    "                                                   'report_keywords'   : {},\n",
    "                                                   'report_score'      : -1,\n",
    "                                                   'report_threshold'  : -1,\n",
    "                                                  },\n",
    "                         'identical_sequences'  : {'databases'         : {},\n",
    "                                                   'output_data'       : [],\n",
    "                                                   'output_file_paths' : [],\n",
    "                                                   'report_keywords'   : {},\n",
    "                                                   'report_score'      : -1,\n",
    "                                                   'report_threshold'  : -1,\n",
    "                                                  },\n",
    "                         'similar_sequences'    : {'databases'         : {},\n",
    "                                                   'output_data'       : [],\n",
    "                                                   'output_file_paths' : [],\n",
    "                                                   'report_keywords'   : {},\n",
    "                                                   'report_score'      : -1,\n",
    "                                                   'report_threshold'  : -1,\n",
    "                                                  },\n",
    "                         'predicted_structures' : {'databases'         : {},\n",
    "                                                   'output_data'       : [],\n",
    "                                                   'output_file_paths' : [],\n",
    "                                                   'report_keywords'   : {},\n",
    "                                                   'report_score'      : -1,\n",
    "                                                   'report_threshold'  : -1,\n",
    "                                                  },\n",
    "                         'similar_structures'   : {'databases'         : {},\n",
    "                                                   'output_data'       : [],\n",
    "                                                   'output_file_paths' : [],\n",
    "                                                   'report_keywords'   : {},\n",
    "                                                   'report_score'      : -1,\n",
    "                                                   'report_threshold'  : -1,\n",
    "                                                  },\n",
    "                        }\n",
    "    \n",
    "    return db\n",
    "\n",
    "\n",
    "\n",
    "# SLURM\n",
    "\n",
    "def get_sjobexitmod(job_id):\n",
    "    return subprocess.check_output(f\"sjobexitmod -l {job_id}\", shell=True, text=True)\n",
    "\n",
    "def parse_sjobexitmod(sjobexitmod_out):\n",
    "    '''\n",
    "    Function to parse output of 'sjobexitmod' process\n",
    "    '''\n",
    "    sjobexitmod_out = sjobexitmod_out.strip().split(\"\\n\")[2:]\n",
    "    \n",
    "    tasks = []\n",
    "    completed_tasks = []\n",
    "    running_tasks = []\n",
    "    pending_tasks = []\n",
    "    error_tasks = []\n",
    "    \n",
    "    for line in sjobexitmod_out:\n",
    "        array_id = line.split()[0]\n",
    "        tasks.append(array_id)\n",
    "        \n",
    "        array_status = line[48:59].strip()\n",
    "        if array_status == \"COMPLETED\":\n",
    "            completed_tasks.append(array_id)\n",
    "        elif array_status == \"RUNNING\":\n",
    "            running_tasks.append(array_id)\n",
    "        elif array_status == \"PENDING\":\n",
    "            pending_tasks.append(array_id)\n",
    "        #elif array_status in [\"ERROR\", \"FAILED\", \"TIMEOUT\"]:\n",
    "        else:\n",
    "            error_tasks.append(array_id)\n",
    "\n",
    "    return tasks, completed_tasks, running_tasks, pending_tasks, error_tasks\n",
    "\n",
    "def execute_sbatch_script(script_path, check_status=True, check_status_sleep=20):\n",
    "    '''\n",
    "    Function to launch an 'sbatch' process\n",
    "    '''    \n",
    "    process = subprocess.Popen(\n",
    "        ['sbatch', script_path],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1,\n",
    "        universal_newlines=True\n",
    "    )\n",
    "\n",
    "    job_id = None  # Initialize the job_id variable\n",
    "    tasks, completed_tasks, running_tasks, pending_tasks, error_tasks = None, None, None, None, None\n",
    "    \n",
    "    while process.poll() is None:\n",
    "        # Obtain the SLURM job id\n",
    "        if job_id is None:\n",
    "            line = process.stdout.readline()\n",
    "            if line:\n",
    "                # Parse the line for job ID if present\n",
    "                if line.startswith(\"Submitted batch job \"):\n",
    "                    job_id = int(line.split()[-1])\n",
    "                    print(f\"SLURM job {job_id} is running.\")\n",
    "                else:\n",
    "                    #print(f\"{line}\", end='')\n",
    "                    pass\n",
    "        else:\n",
    "            if check_status:\n",
    "\n",
    "                slurm_status = get_sjobexitmod(job_id)\n",
    "                tasks, completed_tasks, running_tasks, pending_tasks, error_tasks = parse_sjobexitmod(slurm_status)\n",
    "        \n",
    "                print(f\"Running tasks: {len(running_tasks)}, Completed: {len(completed_tasks)}, Pending: {len(pending_tasks)}, Error/Timeout: {len(error_tasks)} \\t (Checking every {check_status_sleep} s) \", end=\"\\r\")\n",
    "                time.sleep(check_status_sleep) # Check status every n seconds\n",
    "\n",
    "    process.stdout.close()\n",
    "    print(\"\", end=\"\\n\")\n",
    "    return process.returncode, job_id, tasks, completed_tasks, running_tasks, pending_tasks, error_tasks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parsing\n",
    "\n",
    "def tsv_to_dict(tsv_file):\n",
    "    result_dict = {}\n",
    "    \n",
    "    with open(tsv_file, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file, delimiter='\\t')\n",
    "        \n",
    "        for row in reader:\n",
    "            key = row[reader.fieldnames[0]]  # Use the first column as the key\n",
    "            data_dict = {header: row[header] for header in reader.fieldnames}\n",
    "            result_dict[key] = data_dict\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "\n",
    "try:\n",
    "    PHROG_ANNOT_TSV = \"./other_files/phrog_annot_v4.tsv\"\n",
    "    PHROG_DICT = tsv_to_dict(PHROG_ANNOT_TSV)\n",
    "except:\n",
    "    PHROG_DICT = None\n",
    "\n",
    "def parse_hhr(filepath, hhr_type):\n",
    "    if not os.path.isfile(filepath):\n",
    "        return []\n",
    "    \n",
    "    with open(filepath, \"r\") as f:\n",
    "        output_text = f.read()\n",
    "    \n",
    "    main_table = output_text.split(\" No Hit\")[1].split(\"\\n\\nNo \")[0]\n",
    "    single_results = output_text.split(\"\\n\\nNo \")[1:]\n",
    "\n",
    "    results = []\n",
    "    for i, row in enumerate(main_table.split(\"\\n\")[1:]):\n",
    "        row = row.lstrip()\n",
    "        first_space = row.find(\" \")\n",
    "        if first_space > 0:\n",
    "            name_start = first_space + 1\n",
    "            name_width = 30\n",
    "            name_short = row[name_start:name_start+name_width].strip()\n",
    "            \n",
    "            try:\n",
    "                entry_header = single_results[i].split(\"\\n\")[1].lstrip(\">\").strip()\n",
    "            except:\n",
    "                entry_header = \"\"\n",
    "\n",
    "            if hhr_type.lower() == \"pfam\":\n",
    "                entry_id          = name_short.split(\".\")[0]\n",
    "                entry_name        = entry_header.split(\";\")[1].strip()\n",
    "                entry_description = entry_header.split(\";\")[2].strip()\n",
    "                entry_url         = f'https://www.ebi.ac.uk/interpro/entry/pfam/{entry_id}'\n",
    "            elif hhr_type.lower() == \"ncbi-cd\":\n",
    "                entry_id          = name_short.split(\" \")[0].lower()\n",
    "                entry_name        = entry_header.split(\";\")[0].split(\" \")[1].strip()\n",
    "                entry_description = entry_header.strip()\n",
    "                entry_url         = f'https://www.ncbi.nlm.nih.gov/Structure/cdd/cddsrv.cgi?uid={entry_id}'\n",
    "            elif hhr_type.lower() == \"cath\":\n",
    "                entry_id          = name_short.split(\"_\")[1].split(\" \")[0]\n",
    "                entry_name        = entry_header.split(\"| NAME: \")[1].strip().split(\".\")[0].strip()\n",
    "                entry_description = \" \".join(entry_header.split(\"|\")[1:]).strip()\n",
    "                entry_url         = f'https://www.cathdb.info/search?q={entry_id}'\n",
    "            elif hhr_type.lower() == \"uniclust\":\n",
    "                try: #Legacy\n",
    "                    entry_id          = name_short.split(\"|\")[1]\n",
    "                    entry_name        = \" \".join(entry_header.split(\" \")[1:]).split(\"=\")[0][:-2].strip()\n",
    "                    entry_description = entry_name\n",
    "                except: #new\n",
    "                    entry_id          = name_short.split(\" \")[0]\n",
    "                    entry_name        = \" \".join(entry_header.split(\" \")[1:]).split(\" n=\")[0].strip()\n",
    "                    entry_description = entry_name \n",
    "                entry_url         = f'https://www.uniprot.org/uniprotkb/{entry_id.replace(\"UniRef100_\", \"\")}/entry'\n",
    "            elif hhr_type.lower() == \"pdb70\":\n",
    "                entry_id          = name_short.split(\" \")[0]\n",
    "                entry_name        = \" \".join(entry_header.split(\" \")[1:]).split(\";\")[0].strip()\n",
    "                entry_description = \" \".join(entry_header.split(\";\")[:-1]).strip()\n",
    "                entry_url         = f'https://www.rcsb.org/structure/{entry_id.split(\"_\")[0]}'\n",
    "            elif hhr_type.lower() == \"phrogs\":\n",
    "                entry_id          = name_short.split(\" \")[0].split(\"_\")[1]\n",
    "                entry_name        = entry_header.split(\"##\")[0].strip()\n",
    "                entry_description = entry_header.split(\"##\")[1].strip()\n",
    "                entry_url         = f'https://phrogs.lmge.uca.fr/cgi-bin/script_mega_2018.py?mega={entry_id}'\n",
    "\n",
    "                if PHROG_DICT is not None:\n",
    "                    phrog_dict = PHROG_DICT\n",
    "                    phrog_annot = phrog_dict[entry_id]['annot']\n",
    "                    phrog_category = phrog_dict[entry_id]['category']\n",
    "                    entry_description = f\"{phrog_annot}; Category: {phrog_category}; {entry_description}\"\n",
    "                    entry_name = phrog_annot\n",
    "                else:\n",
    "                    entry_name        = entry_header.split(\"##\")[0].strip()\n",
    "                    entry_description = entry_header.split(\"##\")[1].strip()\n",
    "                \n",
    "            else:\n",
    "                entry_id          = name_short\n",
    "                entry_name        = name_short\n",
    "                entry_description = name_short if entry_header == \"\" else entry_header\n",
    "                entry_url         = \"\"\n",
    "            \n",
    "            values     = [v for v in row[name_start+name_width+1:].split(\" \") if v != \"\"]\n",
    "\n",
    "            try:\n",
    "                entry = {\"db\"           : hhr_type,\n",
    "                         \"no\"           : i + 1,\n",
    "                         \"id\"           : entry_id,\n",
    "                         \"name\"         : entry_name,\n",
    "                         \"header\"       : entry_header,\n",
    "                         \"description\"  : entry_description,\n",
    "                         \"url\"          : entry_url,\n",
    "                         \"prob\"         : float(values[0]),\n",
    "                         \"evalue\"       : float(values[1]),\n",
    "                         \"pvalue\"       : float(values[2]),\n",
    "                         \"score\"        : float(values[3]),\n",
    "                         \"ss\"           : float(values[4]),\n",
    "                         \"cols\"         : int(values[5]),\n",
    "                         \"query\"        : tuple([int(v) for v in values[6].split(\"-\")]),\n",
    "                         \"query_len\"    : int(single_results[i].split(\"\\n\")[4].strip().split(\" \")[-1][1:-1]),\n",
    "                         \"template\"     : tuple([int(v) for v in values[7].split(\"(\")[0].split(\"-\")]),\n",
    "                         \"template_len\" : int(single_results[i].split(\"\\n\")[7].strip().split(\" \")[-1][1:-1])}\n",
    "\n",
    "                results.append(entry)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"filepath\", filepath)\n",
    "                print(\"main_table\", main_table)\n",
    "                print(\"entry_name\", entry_name)\n",
    "                print(\"single_results\", single_results)\n",
    "                print(f\"single_results[{i}]\", single_results[i])\n",
    "\n",
    "    return results\n",
    "\n",
    "try:\n",
    "    PDB_SEQRES = \"./other_files/pdb_seqres.txt\"\n",
    "     #seqres = https://files.wwpdb.org/pub/pdb/derived_data/\n",
    "    with open(PDB_SEQRES, \"r\") as f:\n",
    "        pdb_seqres = f.read()\n",
    "    pdb_seqres_list = pdb_seqres.split(\">\")\n",
    "    PDB_SEQRES_DICT = {}\n",
    "    for p in pdb_seqres_list:\n",
    "        header = p.split(\"\\n\")[0]\n",
    "        key = header.split(\" \")[0].upper()\n",
    "        value = \" \".join(header.split(\" \")[3:])\n",
    "        PDB_SEQRES_DICT[key] = value\n",
    "\n",
    "except:\n",
    "    PDB_SEQRES_DICT = None\n",
    "\n",
    "\n",
    "def parse_blasttab(filepath, file_type='pdb'):\n",
    "    if not os.path.isfile(filepath):\n",
    "        return []\n",
    "    \n",
    "    with open(filepath, \"r\") as f:\n",
    "        output_text = f.read()\n",
    "    output_list = output_text.split(\"\\n\")\n",
    "\n",
    "    if output_list[0].startswith(\"query\"):\n",
    "        header = output_list[0]\n",
    "        data = output_list[1:]\n",
    "    else:\n",
    "        data = output_list\n",
    "\n",
    "    results = []\n",
    "    for i, row in enumerate(data):\n",
    "        values = [v.strip() for v in row.split(\"\\t\")]\n",
    "        if len(values) < 12:\n",
    "            continue\n",
    "\n",
    "        if file_type == 'pdb':\n",
    "            entry_name = values[1].replace(\".cif.gz\", \"\")\n",
    "            if \"MODEL\" in entry_name:\n",
    "                entry_name = f\"{entry_name.split('_')[0]}_{entry_name.split('_')[-1]}\"\n",
    "        else:\n",
    "            entry_name = values[1]\n",
    "\n",
    "        entry_id = entry_name.upper()\n",
    "\n",
    "        entry = {\"db\"       : file_type,\n",
    "                 \"no\"       : i + 1,\n",
    "                 \"id\"       : entry_id,\n",
    "                 \"name\"     : entry_name.upper(),\n",
    "                 \"description\" : \"\",\n",
    "                 \"fident\"   : float(values[2]),\n",
    "                 \"alnlen\"   : int(values[3]),\n",
    "                 \"mismatch\" : int(values[4]),\n",
    "                 \"gapopen\"  : int(values[5]),\n",
    "                 \"qstart\"   : int(values[6]),\n",
    "                 \"qend\"     : int(values[7]),\n",
    "                 \"query\"    : (int(values[6]), int(values[7])), # similar to an hhr file\n",
    "                 \"tstart\"   : int(values[8]),\n",
    "                 \"tend\"     : int(values[9]),\n",
    "                 \"evalue\"   : float(values[10]), # similar to an hhr file\n",
    "                 \"bits\"     : int(values[11]),\n",
    "                 \"prob\"     : float(values[12]),\n",
    "                 \"header\"   : str(values[13]),\n",
    "                 \"url\"      : \"\",\n",
    "                }\n",
    "\n",
    "        entry['description'] = \" \".join(entry['header'].split(\" \")[1:]) # TODO: Testing\n",
    "\n",
    "            \n",
    "        if file_type.lower() == 'pdb':\n",
    "            entry[\"url\"] = f'https://www.rcsb.org/structure/{entry_id.split(\"_\")[0]}'\n",
    "            try:\n",
    "                entry[\"description\"] = PDB_SEQRES_DICT[entry_name.upper()]\n",
    "            except:\n",
    "                #print(entry_name)\n",
    "                pass\n",
    "\n",
    "        entry['name'] = entry['description'] # TODO: Testing\n",
    "        results.append(entry)\n",
    "\n",
    "    return results\n",
    "\n",
    "def ranges_overlap(range1, range2, returntype=bool):\n",
    "    x = list(range1)\n",
    "    y = list(range2)\n",
    "    return returntype(range(max(x[0], y[0]), min(x[-1], y[-1])+1))\n",
    "\n",
    "def group_non_overlapping(parsed_hhr):\n",
    "    stack = [[] for x in range(len(parsed_hhr))]\n",
    "    already_placed = []\n",
    "    \n",
    "    for s in range(len(stack)):\n",
    "    \n",
    "        tmp = []\n",
    "        for i in range(len(parsed_hhr)):\n",
    "            no = parsed_hhr[i][\"no\"]\n",
    "            if no not in already_placed:\n",
    "                ro = [ranges_overlap(t[\"query\"], parsed_hhr[i][\"query\"]) for t in tmp]\n",
    "                if True not in ro:\n",
    "                    tmp.append(parsed_hhr[i])\n",
    "                    already_placed.append(no)\n",
    "    \n",
    "        stack[s] = tmp\n",
    "    \n",
    "    stack = [s for s in stack if len(s) != 0]\n",
    "    return stack\n",
    "\n",
    "def evalue_to_color(log_evalue, cmap_name='nipy_spectral'):\n",
    "    \"\"\"\n",
    "    Convert an E-value to a color using a specified colormap.\n",
    "    \n",
    "    Args:\n",
    "        evalue (float): The E-value to convert.\n",
    "        cmap_name (str): Name of the matplotlib colormap to use.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple representing the RGB color.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "\n",
    "    cmap = plt.get_cmap(cmap_name)  # cmaps: https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "    \n",
    "    # Define the mapping of log E-values to the colormap range\n",
    "    log_vmin = np.log10(1e-100)  # Minimum value in log scale\n",
    "    log_vmax = np.log10(1e1)   # Maximum value in log scale\n",
    "    norm = mcolors.Normalize(vmin=log_vmin, vmax=log_vmax)\n",
    "    \n",
    "    # Convert log E-value to normalized value and then to color\n",
    "    color = cmap(norm(np.log10(log_evalue)))\n",
    "    return color\n",
    "\n",
    "def plot_stack(stack, seq_len, output_filepath=None):\n",
    "    if len(stack) == 0:\n",
    "        fig, ax = plt.subplots(figsize=(10,1))\n",
    "        ax.set_xlim(1, seq_len)\n",
    "    else:\n",
    "        fig_height = max(2, len(stack)/2)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10,fig_height))\n",
    "        for i, entries in enumerate(stack):\n",
    "            for e in entries:\n",
    "                e_start = e[\"query\"][0]\n",
    "                e_length = e[\"query\"][1] - e[\"query\"][0]\n",
    "                ax.broken_barh([(e_start, e_length)], (i, 1), facecolor=evalue_to_color(e[\"evalue\"], cmap_name='nipy_spectral'), edgecolor='white')\n",
    "                ax.set_facecolor('white')\n",
    "                ax.text(e_start+e_length/2, i+0.5, f'{e[\"name\"]} \\n {e[\"id\"]}, E={e[\"evalue\"]}',\n",
    "                        ha='center', va='center', color='black', fontsize='x-small',\n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.6, url=e[\"url\"]),\n",
    "                        url=e[\"url\"])\n",
    "\n",
    "        # Change looks\n",
    "        ax.set_xlim(1, seq_len)\n",
    "        ax.set_ylim(-0.1, len(stack)+0.1)\n",
    "\n",
    "        old_xticks = list(ax.get_xticks())\n",
    "        step = old_xticks[1] - old_xticks[0]\n",
    "        new_xticks = sorted(list(np.arange(0, seq_len, step)))# + [1, seq_len]\n",
    "        if new_xticks[0]-1 < step:\n",
    "            new_xticks = new_xticks[1:]\n",
    "        if seq_len-new_xticks[-1] < step:\n",
    "            new_xticks = new_xticks[:-1]\n",
    "        new_xticks = new_xticks + [1, seq_len]\n",
    "        ax.set_xticks(new_xticks)\n",
    "            \n",
    "    # Change looks (common)\n",
    "\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_tick_params(width=2)\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    ## Frame\n",
    "    plt.gca().spines['top'].set_linewidth(2)\n",
    "    plt.gca().spines['top'].set_capstyle(\"round\")\n",
    "    plt.gca().spines['left'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['bottom'].set_visible(False)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "\n",
    "    if output_filepath is None:\n",
    "        plt.plot()\n",
    "        \n",
    "        return \"\"\n",
    "        \n",
    "    else:\n",
    "        plt.savefig(output_filepath, dpi=300, format='svg')\n",
    "        plt.close()\n",
    "        \n",
    "        return output_filepath\n",
    "\n",
    "def plot_combined_stack(combined_stack, seq_len, output_filepath=None):\n",
    "    if len(combined_stack) == 0:\n",
    "        return plot_stack(combined_stack, seq_len, output_filepath)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        cnt = 0\n",
    "        for row in range(len(combined_stack)):\n",
    "            ylabel, stack = combined_stack[row]\n",
    "            cnt += len(stack)\n",
    "            \n",
    "        fig_height = max(len(combined_stack)*2, cnt/2.5)\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=len(combined_stack), ncols=1, figsize=(10,fig_height))\n",
    "\n",
    "        if len(combined_stack) == 1: # If combined_stack is a single item, convert ax to a list. Otherwise, if len(combined_stack) == 1, an error will be raised TypeError: 'Axes' object is not subscriptable\n",
    "            ax = [ax]\n",
    "\n",
    "        for row in range(len(combined_stack)):\n",
    "            ylabel, stack = combined_stack[row]\n",
    "            for i, entries in enumerate(stack):\n",
    "                for e in entries:\n",
    "                    e_start = e[\"query\"][0]\n",
    "                    e_length = e[\"query\"][1] - e[\"query\"][0]\n",
    "                    ax[row].broken_barh([(e_start, e_length)], (i, 1), facecolor=evalue_to_color(e[\"evalue\"], cmap_name='nipy_spectral'), edgecolor='white')\n",
    "            \n",
    "                    ax[row].set_facecolor('white')\n",
    "        \n",
    "                    ax[row].text(e_start+e_length/2, i+0.5, f'{e[\"name\"] if len(e[\"name\"]) < 25 else e[\"name\"][:22]+\"...\"} \\n {e[\"id\"]}, E={e[\"evalue\"]}',\n",
    "                                 ha='center', va='center', color='black', fontsize='x-small',\n",
    "                                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.6, url=e[\"url\"]),\n",
    "                                 url=e[\"url\"])\n",
    "    \n",
    "            # Change plot appearance (common)\n",
    "            ax[row].set_ylim(-0.1, len(stack)+0.1)\n",
    "            ax[row].set_yticks([])\n",
    "            ax[row].invert_yaxis()\n",
    "            ax[row].set_ylabel(ylabel)\n",
    "    \n",
    "            ax[row].spines['left'].set_visible(False)\n",
    "            ax[row].spines['right'].set_visible(False)\n",
    "            ax[row].spines['bottom'].set_visible(False)\n",
    "    \n",
    "            # Change plot appearance (individual)\n",
    "            if row == 0:\n",
    "                # Change X axis appearance\n",
    "                ax[row].set_xlim(1, seq_len)\n",
    "                \n",
    "                old_xticks = list(ax[row].get_xticks())\n",
    "                step = old_xticks[1] - old_xticks[0]\n",
    "                new_xticks = sorted(list(np.arange(0, seq_len, step)))# + [1, seq_len]\n",
    "                if new_xticks[0]-1 < step:\n",
    "                    new_xticks = new_xticks[1:]\n",
    "                if seq_len-new_xticks[-1] < step:\n",
    "                    new_xticks = new_xticks[:-1]\n",
    "                new_xticks = new_xticks + [1, seq_len]\n",
    "                ax[row].set_xticks(new_xticks)\n",
    "                \n",
    "                ax[row].xaxis.tick_top()\n",
    "                ax[row].xaxis.set_tick_params(width=2)\n",
    "    \n",
    "                # Frame\n",
    "                ax[row].spines['top'].set_linewidth(2)\n",
    "                ax[row].spines['top'].set_capstyle(\"round\")\n",
    "     \n",
    "            else:\n",
    "                ax[row].set_xlim(ax[0].get_xlim())\n",
    "                ax[row].get_xaxis().set_visible(False)\n",
    "                ax[row].spines['top'].set_visible(False)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if output_filepath is None:\n",
    "        plt.plot()\n",
    "        return None\n",
    "        \n",
    "    else:\n",
    "        plt.savefig(output_filepath, dpi=300, format='svg')\n",
    "        plt.close()\n",
    "        return output_filepath\n",
    "\n",
    "def combine_dicts(dicts):\n",
    "    combined_dict = {}\n",
    "    \n",
    "    for d in dicts:\n",
    "        for key, value in d.items():\n",
    "            key = key.lower()\n",
    "            if key in combined_dict:\n",
    "                combined_dict[key] += value\n",
    "            else:\n",
    "                combined_dict[key] = value\n",
    "\n",
    "    # Sort the combined dictionary by values in descending order\n",
    "    sorted_combined = dict(sorted(combined_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    return sorted_combined\n",
    "\n",
    "def get_keywords(text, custom_excluded_words = ['protein', 'unknown', 'function', 'subunit', 'domain', 'family', 'category', 'na', 'uncharacterized', 'of', 'to', 'HET', 'CATHCODE', 'NAME', 'SOURCE', 'CLASS', 'HOMOL', 'TOPOL', 'Chain']):\n",
    "\n",
    "    #text = text.lower()\n",
    "    \n",
    "    # Remove custom words\n",
    "    #for word in custom_excluded_words:\n",
    "    #    text = text.replace(f\" {word} \", \"  \")\n",
    "    \n",
    "    # Count word occurrences\n",
    "    words_dict = WordCloud(collocations=False, stopwords=custom_excluded_words).process_text(text)\n",
    "\n",
    "    # Sort the combined dictionary by values in descending order\n",
    "    sorted_words_dict = dict(sorted(words_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    return sorted_words_dict\n",
    "\n",
    "def plot_wordcloud(keywords_dict):\n",
    "\n",
    "    wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(keywords_dict)\n",
    "    \n",
    "    return wordcloud\n",
    "\n",
    "def get_accessions_from_refseq_fasta(fasta_filepath):\n",
    "    # Open and read file\n",
    "    with open(fasta_filepath, \"r\") as f:\n",
    "        fasta = f.read()\n",
    "\n",
    "    # Primary accessions numbers are the ones after the \">\" symbol\n",
    "    # Alternative accession numbers are found after comma + space, i.e. \", \"\n",
    "    # Therefore, make such that each comma + space becomes \">\" to ease accession number recognition\n",
    "    s = fasta.replace(\", \", \"\\n>\")\n",
    "    accessions = []\n",
    "    descriptions = []\n",
    "    organisms = []\n",
    "    for line in s.split(\"\\n\"):\n",
    "        if line.startswith(\">\"):\n",
    "            a = line.split(\" \")[0].replace(\">\", \"\").strip()\n",
    "            d = \" \".join(line.split(\" \")[1:]).strip()\n",
    "\n",
    "            # Weird descriptions coming from UniProt/SwissProt entries\n",
    "            # See table https://www.uniprot.org/release-notes/2008-07-22-release\n",
    "            uniprot_rubbish = [\"RecName:\", \"AltName:\", \"SubName:\", \"Full=\", \"Short=\", \"EC=\", \"Allergen=\", \"Biotech=\", \"CD_antigen=\", \"INN=\"]\n",
    "            for ur in uniprot_rubbish:\n",
    "                d = d.replace(ur, \"\")\n",
    "            if d.endswith(\"]\"):\n",
    "                organism = d.split('[')[1].rstrip(']')\n",
    "                d = d.split(\"[\")[0]\n",
    "            else:\n",
    "                organism = \"\"\n",
    "            d = d.strip().lower().capitalize()\n",
    "            \n",
    "            accessions.append(a)\n",
    "            descriptions.append(d)\n",
    "            organisms.append(organism)\n",
    "\n",
    "    return accessions, descriptions, organisms\n",
    "\n",
    "\n",
    "def extract_b_factors(pdb_file):\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"structure\", pdb_file)\n",
    "    \n",
    "    b_factors = []\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                if residue.get_id()[0] == \" \" and residue.has_id(\"CA\"):\n",
    "                    b_factor = residue[\"CA\"].get_bfactor()\n",
    "                    b_factors.append(b_factor)\n",
    "                    \n",
    "    return b_factors\n",
    "\n",
    "def calculate_distance_matrix(pdb_file):\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"structure\", pdb_file)\n",
    "    \n",
    "    ca_atoms = []\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                if residue.get_id()[0] == \" \" and residue.has_id(\"CA\"):\n",
    "                    ca_atoms.append(residue[\"CA\"].get_coord())\n",
    "    \n",
    "    num_atoms = len(ca_atoms)\n",
    "    distance_matrix = np.zeros((num_atoms, num_atoms))\n",
    "    \n",
    "    for i in range(num_atoms):\n",
    "        for j in range(i + 1, num_atoms):\n",
    "            distance = np.linalg.norm(ca_atoms[i] - ca_atoms[j])\n",
    "            distance_matrix[i, j] = distance\n",
    "            distance_matrix[j, i] = distance\n",
    "    \n",
    "    return distance_matrix\n",
    "\n",
    "def plot_combined_figures(msa, pae_matrix, distance_matrix, b_factors, output_filepath=None):\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    gs = GridSpec(2, 5, width_ratios=[3.5, 0.1, 0.05, 2.5, 0.1], height_ratios=[1, 1], wspace=0.5, hspace=0.3, top=0.95, bottom=0.08, left=0.08, right=0.95)\n",
    "\n",
    "    # Plot MSA in the top-left position\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1_cbar = fig.add_subplot(gs[0, 1])\n",
    "    ax1.set_title(\"Sequence Coverage\")\n",
    "    if msa is None:\n",
    "        ax1.text(0.5, 0.5, 'missing data', verticalalignment='center', horizontalalignment='center', transform=ax1.transAxes, color='red', fontsize=12)\n",
    "    else:\n",
    "        seqid = (np.array(msa[0] == msa).mean(-1))\n",
    "        seqid_sort = seqid.argsort()\n",
    "        non_gaps = (msa != 21).astype(float)\n",
    "        non_gaps[non_gaps == 0] = np.nan\n",
    "        final = non_gaps[seqid_sort] * seqid[seqid_sort, None]\n",
    "        \n",
    "        ax1.imshow(final,\n",
    "                   interpolation='nearest', aspect='auto',\n",
    "                   cmap=\"rainbow_r\", vmin=0, vmax=1, origin='lower')\n",
    "        ax1.plot((msa != 21).sum(0), color='black')\n",
    "        cbar1 = fig.colorbar(cax=ax1_cbar, mappable=ax1.get_images()[0], label=\"Sequence identity to query\")\n",
    "\n",
    "        ax1.set_xlim(-0.5, msa.shape[1] - 0.5)\n",
    "        ax1.set_ylim(-0.5, msa.shape[0] - 0.5)\n",
    "\n",
    "    ax1.set_xlabel(\"Position\")\n",
    "    ax1.set_ylabel(\"Sequences\")\n",
    "    #ax1.set_xlim(0, len(msa[0]))\n",
    "    ax1_cbar.yaxis.set_ticks_position('left')\n",
    "    ax1_cbar.yaxis.set_label_position('left')\n",
    "\n",
    "    # Add spacer\n",
    "    #spacer1 = fig.add_subplot(gs[0, 2])\n",
    "    \n",
    "    # Plot PAE matrix in the top-right position\n",
    "    ax2 = fig.add_subplot(gs[0, 3])\n",
    "    ax2_cbar = fig.add_subplot(gs[0, 4])\n",
    "    ax2.set_title(\"Predicted Aligned Errors\")\n",
    "    if pae_matrix is None:\n",
    "        ax2.text(0.5, 0.5, 'missing data', verticalalignment='center', horizontalalignment='center', transform=ax2.transAxes, color='red', fontsize=12)\n",
    "    else:\n",
    "        plot2 = ax2.matshow(pae_matrix, cmap=plt.cm.Greens.reversed())\n",
    "        cbar2 = fig.colorbar(plot2, cax=ax2_cbar, label=r'Expected position error ($\\AA$)')\n",
    "\n",
    "    ax2.xaxis.tick_bottom()\n",
    "    ax2_cbar.yaxis.set_ticks_position('left')\n",
    "    ax2_cbar.yaxis.set_label_position('left')\n",
    "    ax2.set_xlabel(\"Scored residue\")\n",
    "    ax2.set_ylabel(\"Aligned residue\")\n",
    "\n",
    "        \n",
    "    # Plot pLDDT in the bottom-left position\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    ax3_cbar = fig.add_subplot(gs[1, 1]) # Will stay empty\n",
    "    ax3.set_title(\"Predicted LDDTs\")\n",
    "    if b_factors is None:\n",
    "        ax3.text(0.5, 0.5, 'missing data', verticalalignment='center', horizontalalignment='center', transform=ax3.transAxes, color='red', fontsize=12)\n",
    "    else:\n",
    "        def get_color(b_factor):\n",
    "            if b_factor > 90:\n",
    "                return \"#4287F5\"\n",
    "            elif 70 <= b_factor <= 90:\n",
    "                return \"#AED6F1\"\n",
    "            elif 50 <= b_factor < 70:\n",
    "                return \"#FAD7A0\"\n",
    "            else:\n",
    "                return \"#F59738\"\n",
    "        ax3.axhline(y=np.mean(b_factors), color='#e3e3e3', linewidth=1)\n",
    "        ax3.plot(np.arange(1, len(b_factors) + 1), b_factors, color=\"#000000\", linewidth=1)\n",
    "        ax3.scatter(np.arange(1, len(b_factors) + 1), b_factors, marker=\"o\", c=[get_color(b) for b in b_factors])\n",
    "        # Special colorbar\n",
    "        ## Plot the gradient with custom colors\n",
    "        for start, end, color in [(-50, 50, '#F59738'), (50,  70, '#FAD7A0'), (70,  90, '#AED6F1'), (90, 150, '#4287F5')]:\n",
    "            ax3_cbar.axhspan(start, end, color=color)\n",
    "        ## Set axis limits\n",
    "        ax3_cbar.set_xlim(0, 1)\n",
    "        #ax3_cbar.yaxis.tick_right()\n",
    "        #ax3_cbar.yaxis.set_label_position(\"right\")\n",
    "        ## Hide x-axis and label y-axis\n",
    "        ax3_cbar.set_xticks([])\n",
    "        ax3_cbar.set_ylabel('pLDDT', rotation=90, labelpad=0)\n",
    "        \n",
    "    ax3.set_xlabel(\"Position\")\n",
    "    ax3.set_ylabel('pLDDT')\n",
    "    ax3.set_xlim(0, len(b_factors))\n",
    "    ax3.set_ylim(-5, 105)\n",
    "    ax3_cbar.set_ylim(ax3.get_ylim())\n",
    "\n",
    "    # Add spacer\n",
    "    #spacer2 = fig.add_subplot(gs[1, 2])\n",
    "    \n",
    "    # Plot distance matrix in the bottom-right position\n",
    "    ax4 = fig.add_subplot(gs[1, 3])\n",
    "    ax4_cbar = fig.add_subplot(gs[1, 4])\n",
    "    ax4.set_title(r'C$\\alpha$-C$\\alpha$ Distances')\n",
    "    if distance_matrix is None:\n",
    "        ax4.text(0.5, 0.5, 'missing data', verticalalignment='center', horizontalalignment='center', transform=ax4.transAxes, color='red', fontsize=12)\n",
    "    else:\n",
    "\n",
    "        plot4 = ax4.matshow(distance_matrix, cmap=plt.cm.Purples.reversed())\n",
    "        cbar4 = fig.colorbar(plot4, cax=ax4_cbar, label=r'C$\\alpha$-C$\\alpha$ distance ($\\AA$)')\n",
    "\n",
    "    ax4.xaxis.tick_bottom()\n",
    "    ax4_cbar.yaxis.set_ticks_position('left')\n",
    "    ax4_cbar.yaxis.set_label_position('left')\n",
    "    ax4.set_xlabel(\"Scored residue\")\n",
    "    ax4.set_ylabel(\"Aligned residue\")\n",
    "\n",
    "    #fig.tight_layout()\n",
    "\n",
    "    if output_filepath is None:\n",
    "        plt.show()\n",
    "        return None\n",
    "    else:\n",
    "        plt.savefig(output_filepath, dpi=300, format='svg')\n",
    "        plt.close()\n",
    "        return output_filepath\n",
    "\n",
    "print(\"Initialization OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5207cc-0eb6-4c26-879e-527deb793421",
   "metadata": {},
   "source": [
    "## 2. Data acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff6496-8d78-483b-a77a-ebc10057c5be",
   "metadata": {},
   "source": [
    "### 2.1. Import protein sequences <a id=\"anchor-1\"></a>\n",
    "#### 2.1.1. Specify input files, output directories and important settings\n",
    "- Write your input file path in the `INPUT_FILE` variable in the code block below. The <b>Genbank</b> (.gb, .gbk; it must contain CDSs/features) file format is the preferred choice; however, <u>protein</u> sequence <b>FASTA</b> (.fasta), <b>Excel</b> (.xlsx) and <b>comma-separated values</b> (.csv) are also accepted.  See folder <a href=\"./test_input_files\">./test_input_files</a> for input file examples.\n",
    "- Write your project directory path in the `PROJECT_FOLDER` variable in the code block below. All pipeline outputs will be written into this folder. If the directory does not exist yet, it will be automatically created.\n",
    "- Optional: Change project folder structure (not recommended).\n",
    "- Optional:  Change search algorithm threshold e-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe35a5d-064a-465a-92d2-5a94ebe4cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input file\n",
    "INPUT_FILE =  \"./test_input_files/Escherichia_virus_HeidiAbel.gb\"\n",
    "\n",
    "# Define project folder\n",
    "PROJECT_FOLDER = \"./Escherichia_virus_HeidiAbel\"\n",
    "\n",
    "# Define project folder structure\n",
    "IMPORTED_SEQUENCES_FOLDER   = f\"{PROJECT_FOLDER}/query_sequences\"\n",
    "DOMAIN_ARCHITECTURE_FOLDER  = f\"{PROJECT_FOLDER}/domain_architecture\" \n",
    "SEQUENCE_MATCHING_FOLDER    = f\"{PROJECT_FOLDER}/identical_sequences\"\n",
    "SEQUENCE_SIMILARITY_FOLDER  = f\"{PROJECT_FOLDER}/similar_sequences\"\n",
    "STRUCTURE_PREDICTION_FOLDER = f\"{PROJECT_FOLDER}/predicted_structures\"\n",
    "STRUCTURE_SIMILARITY_FOLDER = f\"{PROJECT_FOLDER}/similar_structures\"\n",
    "REPORT_FOLDER               = f\"{PROJECT_FOLDER}/report\"\n",
    "\n",
    "# Define thresholds for the individual steps\n",
    "THRESHOLDS = {'domain_architecture' : {'evalue' : 1e-3},\n",
    "              'similar_sequences'   : {'evalue' : 1e-3},\n",
    "              'similar_structures'  : {'evalue' : 1e-2}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e1efd-1c56-4a23-849d-462fd40ad1e9",
   "metadata": {},
   "source": [
    "#### 2.1.2. Parse input file and import sequences\n",
    "- The sequences listed in the `INPUT_FILE` are parsed and stored in a dictionary (`db`). Then, they are exported one-by-one as FASTA files in folder `IMPORTED_SEQUENCES_FOLDER` for further processing. Protein sequences imported previously will be <b>overwritten</b> (do not execute the following cell if you do not wish to overwrite existing Python variables and files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae08423-6fbd-4e84-b6dd-45cf7a1ea4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sequences into a variables database 'db'\n",
    "db = import_sequences(INPUT_FILE)\n",
    "print(f\"Protein sequences parsed from '{INPUT_FILE}': {len(db.keys())}\")\n",
    "\n",
    "# Store imported sequences as individual FASTA files.\n",
    "mdir(IMPORTED_SEQUENCES_FOLDER)\n",
    "\n",
    "import_log = \"Number  Unique name                                 AA Sequence                            Len      Locus*\\n\"\n",
    "for i, protein_id in enumerate(db.keys()):\n",
    "    ## Get protein name and sequence from variables database\n",
    "    name     = db[protein_id]['name']\n",
    "    locus    = db[protein_id]['locus']\n",
    "    sequence = db[protein_id]['sequence']\n",
    "\n",
    "    sequence_fragment = sequence[:30]+\"...\" if len(sequence) > 30 else sequence\n",
    "\n",
    "    import_log += \" \".join([str(x) for x in [i+1, \"\".join([\" \" for j in range(6-len(str(i+1)))]),\n",
    "                                             protein_id, \"\".join([\" \" for j in range(42-len(protein_id))]),\n",
    "                                             sequence_fragment, \"\".join([\" \" for j in range(37-len(sequence_fragment))]),\n",
    "                                             str(len(sequence)), \"\".join([\" \" for j in range(7-len(str(len(sequence))))]),\n",
    "                                             locus, \"\".join([\" \" for j in range(30-len(locus))]), \"\\n\"]])\n",
    "\n",
    "    ## Save to file\n",
    "    fasta_filepath = f\"{IMPORTED_SEQUENCES_FOLDER}/{protein_id}.fasta\"\n",
    "    with open(fasta_filepath, \"w\") as f:\n",
    "        f.write(f\">{name}\\n{sequence}\")\n",
    "\n",
    "    ## Add the saved file path to the database\n",
    "    db[protein_id]['sequence_file_path'] = fasta_filepath\n",
    "\n",
    "# Export log file\n",
    "with open(f\"{IMPORTED_SEQUENCES_FOLDER}/import.log\", \"w\") as f:\n",
    "    f.write(import_log)\n",
    "\n",
    "print(f\"Protein sequences exported to '{IMPORTED_SEQUENCES_FOLDER}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acddaa2d-f335-4ef7-93f3-4601b3f71e77",
   "metadata": {},
   "source": [
    "### 2.2. Determine domain architecture and superfamily\n",
    "\n",
    "Each imported sequence is searched with HHblits (HH-suite) in each one of these databases:\n",
    "\n",
    "- Pfam\n",
    "- NCBI-CD\n",
    "- CATH\n",
    "- PHROGs\n",
    "\n",
    "<div style=\"background-color: #e1f0ff; padding: 12px; border-radius: 7px; box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.1);\">\n",
    "     <b>Information:</b> The following code block executes a resource-intensive process. If output files for a particular sequence and searched database already exist, the existing results will be preserved, and HHblits will not be executed again for those sequences.  \n",
    "To perform fresh calculations on sequences that have already been processed, please manually remove the corresponding output files or directories before rerunning this code block.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c4b81-e508-4a1b-a98f-b8325011a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which databases to search with HHblits and their location\n",
    "databases_to_search = {\"pfam\"    : \"/your/own/database/folder/Pfam35/pfama\",\n",
    "                       \"ncbi-cd\" : \"/your/own/database/folder/NCBI_CD_v3.19/NCBI_CD\",\n",
    "                       \"cath\"    : \"/your/own/database/folder/CATH/CATH_S40\",\n",
    "                       \"phrogs\"  : \"/your/own/database/folder/phrogs/phrogs\",\n",
    "                      }\n",
    "\n",
    "# Make output folders\n",
    "output_dir     = mdir(DOMAIN_ARCHITECTURE_FOLDER)\n",
    "slurm_logs_dir = mdir(f\"{output_dir}/slurm_logs\")\n",
    "\n",
    "# Refresh db items\n",
    "for protein_id in db.keys():\n",
    "    db[protein_id]['domain_architecture']['databases'] = databases_to_search\n",
    "    db[protein_id]['domain_architecture']['output_data'] = []\n",
    "    db[protein_id]['domain_architecture']['output_file_paths'] = []\n",
    "    \n",
    "\n",
    "# Compile a SLURM script to run HHblits for each protein sequence and database in the above list\n",
    "## Write SLURM batch array commands\n",
    "slurm_array_commands = []\n",
    "for database_name, database_path in databases_to_search.items():\n",
    "    for protein_id in db.keys():\n",
    "        input_file_path = db[protein_id]['sequence_file_path']\n",
    "        output_file_path = f\"{output_dir}/{protein_id}_{database_name}.hhr\"\n",
    "\n",
    "        # Add expected output filepath to db\n",
    "        db[protein_id]['domain_architecture']['output_file_paths'].append(output_file_path)\n",
    "        \n",
    "        if not os.path.isfile(output_file_path):\n",
    "            # If expected output file does not exist yet, add command to the commands list;\n",
    "            # If expected output file already exists, don't add it again to the commands list.\n",
    "            # This allows to re-run HHblits with only the sequences that are missing, or\n",
    "            # that failed/timed out in a previous SLURM execution.\n",
    "            command = f\"\"\" hhblits -i   {input_file_path} \n",
    "                                   -d   {database_path} \n",
    "                                   -o   {output_file_path}\n",
    "                                   -cpu 12\n",
    "                                   -n   3\n",
    "                                   -e   0.001\n",
    "                                   -E   10\n",
    "                                   -Z   500\n",
    "                      \"\"\"\n",
    "\n",
    "            # Remove unnecessary spaces (very important, otherwise SLURM will fail!)\n",
    "            command = ' '.join(command.split())\n",
    "\n",
    "            # Finally, add to list\n",
    "            slurm_array_commands.append(command)\n",
    "            print(f\"Entry added to the SLURM queue: {protein_id}, database: {database_name}\")\n",
    "        \n",
    "\n",
    "\n",
    "if len(slurm_array_commands) > 0:\n",
    "    \n",
    "    ## Save commands to file\n",
    "    slurm_cmd_filepath = f\"{output_dir}/slurm.cmd\"\n",
    "    with open(slurm_cmd_filepath, \"w\") as f:\n",
    "        f.write(\"\\n\".join(slurm_array_commands).strip())\n",
    "    print(f\"New file created: {slurm_cmd_filepath}\")\n",
    "\n",
    "    ## Write SLURM script\n",
    "    slurm_arrays_n = len(slurm_array_commands)\n",
    "    slurm_job_id = random.randint(1, 999999)\n",
    "    slurm_script = f\"\"\"#!/bin/sh \n",
    "\n",
    "    #SBATCH --job-name=hhblits_array_job_{slurm_job_id}\n",
    "    #SBATCH --time=0-01:00:00\n",
    "    #SBATCH --qos=6hours\n",
    "    #SBATCH --output={slurm_logs_dir}/slurm%A_%a.out\n",
    "    #SBATCH --error={slurm_logs_dir}/slurm%A_%a.log\n",
    "    #SBATCH --mem=32G\n",
    "    #SBATCH --array=1-{slurm_arrays_n}%{min(24,slurm_arrays_n)}\n",
    "    #SBATCH --cpus-per-task=12\n",
    "    #SBATCH --partition=scicore\n",
    "    #SBATCH --wait\n",
    "    \n",
    "    module load HH-suite\n",
    "    \n",
    "    $(head -$SLURM_ARRAY_TASK_ID {slurm_cmd_filepath} | tail -1)\n",
    "                       \n",
    "    \"\"\"\n",
    "\n",
    "    # Remove any leading spaces (very important, otherwise SLURM will fail!)\n",
    "    slurm_script = \"\\n\".join([s.lstrip() for s in slurm_script.split(\"\\n\")])\n",
    "\n",
    "    ## Save script to file\n",
    "    slurm_sh_filepath = f\"{output_dir}/slurm.sh\"\n",
    "    with open(slurm_sh_filepath, \"w\") as f:\n",
    "        f.write(slurm_script.strip())\n",
    "    print(f\"New file created: {slurm_sh_filepath}\")\n",
    "\n",
    "    \n",
    "    # Execute SLURM batch job\n",
    "    print(f\"Launching SLURM batch script: {slurm_sh_filepath}\")\n",
    "    print(f\"Planned tasks: {len(slurm_array_commands)} total.\")\n",
    "    \n",
    "    returncode, job_id, tasks, completed_tasks, running_tasks, pending_tasks, error_tasks = execute_sbatch_script(slurm_sh_filepath, check_status=True, check_status_sleep=15)\n",
    "    \n",
    "    if returncode == 0:\n",
    "        print(f\"SLURM job {job_id} completed.\")\n",
    "    else:\n",
    "        print(f\"SLURM batch job {job_id} encountered some problems. Exit code: {returncode}. Please read the SLURM documentation to learn more\")\n",
    "    \n",
    "    # Print SLURM summary\n",
    "    slurm_status = get_sjobexitmod(job_id)\n",
    "    print(\"Summary:\\n\\n\" + slurm_status)\n",
    "                  \n",
    "else:\n",
    "    print(\"All scheduled output files already exist!\")\n",
    "    print(f\"Manually remove files from '{output_dir}' if you wish to repeat domain motif search with HHblits.\")\n",
    "\n",
    "\n",
    "    \n",
    "# Parse HHblits output files\n",
    "\n",
    "print(\"Parsing HHblits output files...\")\n",
    "\n",
    "threshold_key   = list(THRESHOLDS['domain_architecture'].keys())[0]\n",
    "threshold_value = THRESHOLDS['domain_architecture'][threshold_key]\n",
    "\n",
    "print(f\"Applying {threshold_key} threshold at {threshold_value}...\")\n",
    "\n",
    "for protein_id in db.keys():\n",
    "    filepath_list = db[protein_id]['domain_architecture']['output_file_paths']\n",
    "\n",
    "    db[protein_id]['domain_architecture']['output_data'] = []\n",
    "\n",
    "    descriptions = []\n",
    "    stacks = []\n",
    "    \n",
    "    for filepath in filepath_list:\n",
    "        if filepath.endswith(\".hhr\"):\n",
    "            \n",
    "            hhr_type = filepath.split(\"_\")[-1].split(\".\")[0]\n",
    "            parsed_hhr = parse_hhr(filepath, hhr_type)\n",
    "\n",
    "            # Score for report\n",
    "            if len(parsed_hhr) > 0:\n",
    "                minval = np.min([p[\"evalue\"] for p in parsed_hhr])\n",
    "                if db[protein_id]['domain_architecture']['report_score'] == -1 or minval < db[protein_id]['domain_architecture']['report_score']:\n",
    "                    db[protein_id]['domain_architecture']['report_score'] = minval\n",
    "\n",
    "            # Apply threshold\n",
    "            \n",
    "            if threshold_key == 'evalue':\n",
    "                parsed_hhr = [p for p in parsed_hhr if p[\"evalue\"] <= threshold_value]\n",
    "            db[protein_id]['domain_architecture']['report_threshold'] = threshold_value\n",
    "\n",
    "            for phhr in parsed_hhr:\n",
    "                descriptions.append(phhr['description'].replace(\"-\", \"_\"))\n",
    "\n",
    "            db[protein_id]['domain_architecture']['output_data'] += parsed_hhr\n",
    "            \n",
    "            # Individual plots\n",
    "            #stack         = group_non_overlapping(parsed_hhr)[:10]\n",
    "            #plot_filepath = plot_stack(stack, len(db[protein_id]['sequence']), filepath.replace(\".hhr\", \".svg\"))\n",
    "            \n",
    "            #if plot_filepath not in db[protein_id]['domain_architecture']['output_file_paths']:\n",
    "            #    db[protein_id]['domain_architecture']['output_file_paths'].append(plot_filepath)\n",
    "\n",
    "            # Same plot\n",
    "            stack = group_non_overlapping(parsed_hhr) # Recalculate stack after threshold application\n",
    "            if len(stack) != 0:\n",
    "                stacks.append((filepath.split(\"_\")[-1].split(\".\")[0], stack[:5]))\n",
    "\n",
    "    # Plot all HHRs in a single figure\n",
    "    combined_plot_filepath = plot_combined_stack(stacks, len(db[protein_id]['sequence']), \"_\".join(filepath.split(\"_\")[:-1])+\"_merged.svg\")\n",
    "    if combined_plot_filepath not in db[protein_id]['domain_architecture']['output_file_paths']:\n",
    "        db[protein_id]['domain_architecture']['output_file_paths'].append(combined_plot_filepath)\n",
    "\n",
    "    db[protein_id]['domain_architecture']['report_keywords'] = get_keywords(\" \".join(descriptions))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3ee4c-2d79-487e-9893-774f24b9bc68",
   "metadata": {},
   "source": [
    "### 2.3. Find identical proteins in sequence/structure databases\n",
    "\n",
    "The pipeline checks if any of the imported protein sequences has an exact match in the multi-FASTA \"databases\" (same as used for BLAST). If yes, existing annotations are added to the project.\n",
    "Searched databases:\n",
    "\n",
    "- PDB\n",
    "- SwissProt\n",
    "- RefSeq\n",
    "\n",
    "<div style=\"background-color: #e1f0ff; padding: 12px; border-radius: 7px; box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.1);\">\n",
    "     <b>Information:</b> The following code block is a resource-consuming process. Therefore, if the output directory `SEQUENCE_MATCHING_FOLDER` already exists, the search will not be executed. If you wish to run new searches for already-processed sequences, remove the existing output folder manually.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8832f98e-c174-466f-a72c-7b5eab8e78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Consider using BLAST instead of textual search\n",
    "\n",
    "# Specify which multi-sequence FASTA files to check for proteins identical to queries.\n",
    "# Provide them as a list of tuples, e.g. [(description1, filepath1), ...]\n",
    "databases_to_search = {\"pdb\"       : \"/your/own/database/folder/BLAST_FASTA/FASTA/pdbaa\",      # this is a format-less FASTA file\n",
    "                       \"swissprot\" : \"/your/own/database/folder/BLAST_FASTA/FASTA/swissprot\",  # this is a format-less FASTA file\n",
    "                       \"refseq\"    : \"/your/own/database/folder/BLAST_FASTA/FASTA/nr\"}         # this is a format-less FASTA file\n",
    "\n",
    "# Define output directory\n",
    "output_dir = SEQUENCE_MATCHING_FOLDER\n",
    "\n",
    "# Refresh db items\n",
    "for protein_id in db.keys():\n",
    "    db[protein_id]['identical_sequences']['databases'] = databases_to_search\n",
    "    db[protein_id]['identical_sequences']['output_file_paths'] = []\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "\n",
    "    # Make output files' folder\n",
    "    output_dir = mdir(output_dir)\n",
    "    \n",
    "    # Function used to find protein sequences identical to query\n",
    "    def find_matches(filepath, query_sequences_list):\n",
    "        # Slower method than loading each file into memory first (although much\n",
    "        # quicker than parsing every entry with biopython), but capable of\n",
    "        # handling very heavy files just fine\n",
    "    \n",
    "        query_sequences_set = set(query_sequences_list)\n",
    "        \n",
    "        matches = [[] for _ in range(len(query_sequences_list))]\n",
    "        \n",
    "        with open(filepath, \"r\") as file:\n",
    "            header = \"\"\n",
    "            dynamic_sequence = \"\"\n",
    "            \n",
    "            for line in file:\n",
    "                line = line.rstrip(\"\\n\")\n",
    "                if line.startswith(\">\"):\n",
    "                    if dynamic_sequence in query_sequences_set:\n",
    "                        \n",
    "                        matched_indexes = [i for i, query_sequence in enumerate(query_sequences_list) if query_sequence == dynamic_sequence]\n",
    "    \n",
    "                        new_header = header.strip().replace(\"\\x01\", \", \")\n",
    "                        if not new_header.startswith(\">\"):\n",
    "                            new_header = \">\"+new_header\n",
    "                        \n",
    "                        for i in matched_indexes:\n",
    "                            matches[i].append(f\"{new_header}\\n{dynamic_sequence}\")\n",
    "                    header = line\n",
    "                    dynamic_sequence = \"\"\n",
    "                else:\n",
    "                    dynamic_sequence += line\n",
    "        \n",
    "        return matches       \n",
    "    \n",
    "    # Get list of imported protein names\n",
    "    protein_ids = list(db.keys())\n",
    "    \n",
    "    # Put all imported sequences into a list\n",
    "    query_sequences = [db[protein_id]['sequence'] for protein_id in protein_ids]\n",
    "        \n",
    "    # Process FASTA files in 'queue' sequentially\n",
    "    for database_name, database_path in databases_to_search.items():\n",
    "\n",
    "        print(f\"Checking for identical proteins in '{database_name}' ('{database_path}') ...\")\n",
    "        \n",
    "        # Search all imported sequences for identical proteins in the current file\n",
    "        # Returns a list with shape equal to 'query_sequences' (also same shape as 'protein_ids')\n",
    "        matched_sequences = find_matches(database_path, query_sequences)\n",
    "        \n",
    "        count = 0\n",
    "        for j, protein_id in enumerate(protein_ids):\n",
    "            if len(matched_sequences[j]) > 0:\n",
    "                # Specify output file path\n",
    "                out_filepath = f\"{output_dir}/{protein_id}_{database_name}.fasta\"\n",
    "                \n",
    "                # Save matched sequences to file (FASTA format)\n",
    "                with open(out_filepath, \"w\") as g:\n",
    "                    g.write(\"\\n\".join(matched_sequences[j]))\n",
    "    \n",
    "                db[protein_id]['identical_sequences']['output_file_paths'].append(out_filepath)\n",
    "                \n",
    "                count += 1\n",
    "            else:\n",
    "                out_filepath = \"\"\n",
    "    \n",
    "    \n",
    "        print(f\"Done! Imported protein sequences matched to existing entries: {count}\")\n",
    "    \n",
    "    print(f\"Output files saved in: '{output_dir}'.\")\n",
    "\n",
    "else:    \n",
    "    print(\"Output folder already exist. To save time and resources, the search of identical proteins will not be run.\")\n",
    "    print(f\"Manually remove folder '{output_dir}' if you wish to repeat the search.\")\n",
    "    print(\"Information will be loaded into local memory from the existing files...\")\n",
    "    \n",
    "    files = [f\"{output_dir}/{f}\" for f in os.listdir(output_dir) if f.endswith(\".fasta\")]\n",
    "    \n",
    "    for f in files:\n",
    "        basename = os.path.basename(f)\n",
    "        basename = basename.split(\".fasta\")[0]\n",
    "        suffix = basename.split(\"_\")[-1]\n",
    "        protein_id = basename.split(f\"_{suffix}\")[0]\n",
    "        try:\n",
    "            db[protein_id]['identical_sequences']['output_file_paths'].append(f)\n",
    "        except:\n",
    "            print(f\"File '{f}' could not be read. (id, suffix)\")\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "# Parse output\n",
    "\n",
    "print(\"Parsing output files...\")\n",
    "\n",
    "for protein in db.keys():\n",
    "\n",
    "    # Analyse identical proteins\n",
    "    \n",
    "    db[protein]['identical_sequences']['output_data'] = []\n",
    "    db[protein]['identical_sequences']['report_score'] = 0\n",
    "\n",
    "    all_descriptions = []\n",
    "    for f in db[protein]['identical_sequences']['output_file_paths']:\n",
    "        basename = os.path.basename(f)\n",
    "        db_name = basename.split(\"_\")[-1].split(\".fasta\")[0]\n",
    "\n",
    "        accessions, descriptions, organisms = get_accessions_from_refseq_fasta(f)\n",
    "        all_descriptions += descriptions\n",
    "        db[protein]['identical_sequences']['output_data'].append((db_name, accessions, descriptions, organisms))\n",
    "        db[protein]['identical_sequences']['report_score'] += len(accessions)\n",
    "\n",
    "    db[protein]['identical_sequences']['report_keywords'] = get_keywords(\" \".join(all_descriptions))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbd4cec-4bef-4f57-b04c-72ea60a57a4a",
   "metadata": {},
   "source": [
    "### 2.4. Sequence similarity search\n",
    "\n",
    "Each imported sequence is searched with HHblits (HH-suite) in each one of these databases:\n",
    "\n",
    "- Uniclust30 / Uniref30\n",
    "- PDB70\n",
    "\n",
    "<div style=\"background-color: #e1f0ff; padding: 12px; border-radius: 7px; box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.1);\">\n",
    "     <b>Information:</b> The following code block executes a resource-intensive process. If output files for a particular sequence and searched database already exist, the existing results will be preserved, and HHblits will not be executed again for those sequences.  \n",
    "To perform fresh calculations on sequences that have already been processed, please manually remove the corresponding output files or directories before rerunning this code block.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747d7bc-a0a2-4d87-a25a-e6868017d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which databases to use for HHblits. Provide them in a dict {name1 : path1, name2 : path2, ...}\n",
    "databases_to_search = {\"uniclust\" : \"/your/own/database/folder/Uniclust/UniRef30/UniRef30_2023_02\",\n",
    "                       \"pdb70\"    : \"/your/own/database/folder/PDB70/pdb70\"}\n",
    "\n",
    "# Databases that could be added in the future\n",
    "## Virulence Factors of Pathogenic bacteria\n",
    "### http://www.mgc.ac.cn/VFs/\n",
    "## Comprehensive Antibiotic Resistance Database\n",
    "### https://card.mcmaster.ca/\n",
    "\n",
    "# Make output folders\n",
    "output_dir     = mdir(SEQUENCE_SIMILARITY_FOLDER)\n",
    "slurm_logs_dir = mdir(f\"{output_dir}/slurm_logs\")\n",
    "\n",
    "# Add items to db\n",
    "for protein_id in db.keys():\n",
    "    db[protein_id]['similar_sequences']['databases'] = databases_to_search\n",
    "    db[protein_id]['similar_sequences']['output_file_paths'] = []\n",
    "\n",
    "# Compile a SLURM script to run HHblits for each protein sequence and database in the above list\n",
    "## Write SLURM batch array commands\n",
    "slurm_array_commands = []\n",
    "for database_name, database_path in databases_to_search.items():\n",
    "    for protein_id in db.keys():\n",
    "        input_file_path   = db[protein_id]['sequence_file_path']\n",
    "        output_file_path1 = f\"{output_dir}/{protein_id}_{database_name}.hhr\"\n",
    "        output_file_path2 = f\"{output_dir}/{protein_id}_{database_name}.a3m\"\n",
    "\n",
    "        # Add expected output filepaths to db\n",
    "        db[protein_id]['similar_sequences']['output_file_paths'].append(output_file_path1)\n",
    "        db[protein_id]['similar_sequences']['output_file_paths'].append(output_file_path2)\n",
    "        \n",
    "        if os.path.isfile(output_file_path1) and os.path.isfile(output_file_path2):\n",
    "            # If expected output file already exists, don't add it again to the commands list.\n",
    "            # This allows to re-run HHblits with only the sequences that are missing, or\n",
    "            # that failed/timed out in a previous SLURM execution.\n",
    "            pass\n",
    "        else:\n",
    "            # If expected output file does not exist yet, add command to the commands list\n",
    "            command = f\"\"\" hhblits -i    {input_file_path} \n",
    "                                   -d    {database_path} \n",
    "                                   -o    {output_file_path1}\n",
    "                                   -oa3m {output_file_path2}\n",
    "                                   -cpu  12\n",
    "                                   -n    3\n",
    "                                   -e    0.001\n",
    "                                   -E    10\n",
    "                                   -Z    500\n",
    "                       \"\"\"\n",
    "\n",
    "            # Remove newlines (very important, otherwise SLURM will fail!)\n",
    "            command = ' '.join(command.split())\n",
    "\n",
    "            # Add to list\n",
    "            slurm_array_commands.append(command)\n",
    "            print(f\"Sequence added to the SLURM queue: {protein_id}, database: {database_name}\")\n",
    "\n",
    "            \n",
    "\n",
    "if len(slurm_array_commands) > 0:\n",
    "    \n",
    "    ## Save commands to file\n",
    "    slurm_cmd_filepath = f\"{output_dir}/slurm.cmd\"\n",
    "    with open(slurm_cmd_filepath, \"w\") as f:\n",
    "        f.write(\"\\n\".join(slurm_array_commands).strip())\n",
    "    print(f\"New file created: {slurm_cmd_filepath}\")\n",
    "\n",
    "    ## Write SLURM script\n",
    "    slurm_arrays_n = len(slurm_array_commands)\n",
    "    slurm_job_id = random.randint(1, 999999)\n",
    "    slurm_script = f\"\"\"#!/bin/sh \n",
    "\n",
    "    #SBATCH --job-name=hhblits_array_job_{slurm_job_id}\n",
    "    #SBATCH --time=0-01:00:00\n",
    "    #SBATCH --qos=6hours\n",
    "    #SBATCH --output={slurm_logs_dir}/slurm%A_%a.out\n",
    "    #SBATCH --error={slurm_logs_dir}/slurm%A_%a.log\n",
    "    #SBATCH --mem=32G\n",
    "    #SBATCH --array=1-{slurm_arrays_n}%{min(24,slurm_arrays_n)}\n",
    "    #SBATCH --cpus-per-task=12\n",
    "    #SBATCH --partition=scicore\n",
    "    #SBATCH --wait\n",
    "    \n",
    "    module load HH-suite\n",
    "    \n",
    "    $(head -$SLURM_ARRAY_TASK_ID {slurm_cmd_filepath} | tail -1)\n",
    "                       \n",
    "    \"\"\"\n",
    "\n",
    "    # Remove any leading spaces (very important, otherwise SLURM will fail!)\n",
    "    slurm_script = \"\\n\".join([s.lstrip() for s in slurm_script.split(\"\\n\")])\n",
    "\n",
    "    ## Save script to file\n",
    "    slurm_sh_filepath = f\"{output_dir}/slurm.sh\"\n",
    "    with open(slurm_sh_filepath, \"w\") as f:\n",
    "        f.write(slurm_script.strip())\n",
    "    print(f\"New file created: {slurm_sh_filepath}\")\n",
    "\n",
    "    \n",
    "    # Execute SLURM batch job\n",
    "    print(f\"Launching SLURM batch script: {slurm_sh_filepath}\")\n",
    "    print(f\"Planned tasks: {len(slurm_array_commands)} total.\")\n",
    "    \n",
    "    returncode, job_id, tasks, completed_tasks, running_tasks, pending_tasks, error_tasks = execute_sbatch_script(slurm_sh_filepath, check_status=True, check_status_sleep=15)\n",
    "    \n",
    "    if returncode == 0:\n",
    "        print(f\"SLURM job {job_id} completed.\")\n",
    "    else:\n",
    "        print(f\"SLURM batch job {job_id} encountered some problems. Exit code: {returncode}. Please read the SLURM documentation to learn more\")\n",
    "    \n",
    "    # Print SLURM summary\n",
    "    slurm_status = get_sjobexitmod(job_id)\n",
    "    print(\"Summary:\\n\\n\" + slurm_status)\n",
    "\n",
    "                  \n",
    "else:\n",
    "    print(\"All scheduled output files already exist!\")\n",
    "    print(f\"Manually remove files from '{output_dir}' if you wish to repeat structure similarity search with HHblits.\")\n",
    "\n",
    "\n",
    "\n",
    "# Parse HHblits output files\n",
    "\n",
    "print(\"Parsing HHblits output files...\")\n",
    "\n",
    "threshold_key   = list(THRESHOLDS['similar_sequences'].keys())[0]\n",
    "threshold_value = THRESHOLDS['similar_sequences'][threshold_key]\n",
    "print(f\"Applying {threshold_key} threshold at {threshold_value}...\")\n",
    "\n",
    "count = 0\n",
    "for protein_id in db.keys():\n",
    "    filepath_list = db[protein_id]['similar_sequences']['output_file_paths']\n",
    "\n",
    "    db[protein_id]['similar_sequences']['output_data'] = []\n",
    "\n",
    "    descriptions = []\n",
    "    stacks = []\n",
    "    \n",
    "    for filepath in filepath_list:\n",
    "        if filepath.endswith(\".hhr\"):\n",
    "            \n",
    "            # Parse HHR file\n",
    "            hhr_type = filepath.split(\"_\")[-1].split(\".\")[0]\n",
    "            parsed_hhr = parse_hhr(filepath, hhr_type)\n",
    "            \n",
    "             # Score for report\n",
    "            if len(parsed_hhr) > 0:\n",
    "                minval = np.min([p[\"evalue\"] for p in parsed_hhr])\n",
    "                if db[protein_id]['similar_sequences']['report_score'] == -1 or minval < db[protein_id]['similar_sequences']['report_score']:\n",
    "                    db[protein_id]['similar_sequences']['report_score'] = minval\n",
    "            \n",
    "            # Plot the HHRs separately for each .hhr file\n",
    "            #stack = group_non_overlapping(parsed_hhr)#[:50]\n",
    "            #plot_filepath = plot_stack(stack, len(db[protein_id]['sequence']), filepath.replace(\".hhr\",\".svg\"))\n",
    "            #if plot_filepath not in db[protein_id]['similar_sequences']['output_file_paths']:\n",
    "            #    db[protein_id]['similar_sequences']['output_file_paths'].append(plot_filepath)\n",
    "\n",
    "            # Apply threshold\n",
    "            if threshold_key == 'evalue':\n",
    "                parsed_hhr = [p for p in parsed_hhr if p[\"evalue\"] <= threshold_value]\n",
    "            db[protein_id]['similar_sequences']['report_threshold'] = threshold_value\n",
    "            \n",
    "            # Fetch description to generate keywords\n",
    "            for phhr in parsed_hhr:\n",
    "                descriptions.append(phhr['description'].replace(\"-\", \"_\"))\n",
    "\n",
    "            db[protein_id]['similar_sequences']['output_data'] += parsed_hhr\n",
    "            \n",
    "            # Stack the HHRs for a combined plot\n",
    "            stack = group_non_overlapping(parsed_hhr) # Recalculate stack after threshold application\n",
    "            if len(stack) != 0:\n",
    "                stacks.append((filepath.split(\"_\")[-1].split(\".\")[0], stack[:10]))\n",
    "\n",
    "\n",
    "    # Plot all HHRs in a single figure\n",
    "    combined_plot_filepath = plot_combined_stack(stacks, len(db[protein_id]['sequence']), \"_\".join(filepath.split(\"_\")[:-1])+\"_merged.svg\")\n",
    "    if combined_plot_filepath not in db[protein_id]['similar_sequences']['output_file_paths']:\n",
    "            db[protein_id]['similar_sequences']['output_file_paths'].append(combined_plot_filepath)\n",
    "\n",
    "    db[protein_id]['similar_sequences']['report_keywords'] = get_keywords(\" \".join(descriptions))\n",
    "\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b0f23-5920-4511-a972-b83a2159fdbf",
   "metadata": {},
   "source": [
    "### 2.5. Structure prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b87f229-243c-423d-8476-d6275912b95e",
   "metadata": {},
   "source": [
    "The putative structure of each imported protein sequence will be predicted by AlphaFold.\n",
    "\n",
    "<div style=\"background-color: #e1f0ff; padding: 12px; border-radius: 7px; box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.1);\">\n",
    "     <b>Information:</b> If the output folder for a certain sequence already exists, its current content will be maintaned and AlphaFold will not be executed. If you wish to run new AlphaFold predictions for protein sequences that were already processed, remove the existing output files/folders manually.</br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a173e-b959-4fc8-9140-e5768ae65419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify alternative obsolete.dat file (optional)\n",
    "## More information to be found in './other_files/modify_obsolete.dat.ipynb'\n",
    "NEW_OBSOLETE_FILEPATH = \"./other_files/obsolete_new.dat\"\n",
    "\n",
    "# Make output folders\n",
    "output_dir     = mdir(STRUCTURE_PREDICTION_FOLDER, chmod=777)\n",
    "slurm_logs_dir = mdir(f\"{output_dir}/slurm_logs\")\n",
    "\n",
    "# Add items to db\n",
    "for protein_id in db.keys():\n",
    "    db[protein_id]['predicted_structures']['output_file_paths'] = []\n",
    "\n",
    "\n",
    "# Compile a SLURM script to run AlphaFold for each protein sequence in the database\n",
    "## Write SLURM batch array commands\n",
    "commands = []\n",
    "for protein_id in db.keys():\n",
    "    input_filepath = db[protein_id]['sequence_file_path']\n",
    "    output_file_path1 = f\"{output_dir}/{protein_id}/ranked_0.pdb\"\n",
    "    output_file_path2 = f\"{output_dir}/{protein_id}/features.pkl\"\n",
    "    output_file_path3 = f\"{output_dir}/{protein_id}/result_model_1_ptm_pred_0.pkl\"\n",
    "    \n",
    "    # Add expected output filepaths to db\n",
    "    db[protein_id]['predicted_structures']['output_file_paths'].append(output_file_path1)\n",
    "    db[protein_id]['predicted_structures']['output_file_paths'].append(output_file_path2)\n",
    "    db[protein_id]['predicted_structures']['output_file_paths'].append(output_file_path3)\n",
    "\n",
    "    if os.path.isfile(output_file_path1):\n",
    "        # If expected output file already exists, don't add it again to the commands file.\n",
    "        # This allows to re-run AlphaFold with only the structures that are missing, or\n",
    "        # that failed/timed out in a previous SLURM execution.\n",
    "        \n",
    "        # print(f\"File '{expected_output_file}' already exists.\")\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        command = f\"\"\"\n",
    "        run_alphafold.sh\n",
    "        --fasta_paths={input_filepath}\n",
    "        --output_dir={output_dir}\n",
    "        --use_gpu_relax=true\n",
    "        --model_preset=monomer_ptm\n",
    "        --max_template_date=2021-09-01\n",
    "        {\"--obsolete_pdbs_path=\"+NEW_OBSOLETE_FILEPATH if os.path.isfile(NEW_OBSOLETE_FILEPATH) else \"\"}\n",
    "        \"\"\"\n",
    "\n",
    "        # Remove newlines (very important, otherwise SLURM will fail!)\n",
    "        command = ' '.join(command.split())\n",
    "        \n",
    "        # Add to list\n",
    "        commands.append(command)\n",
    "        print(f\"Sequence added to the SLURM queue: {protein_id}\")\n",
    "\n",
    "        \n",
    "## Check if there are any new commands (i.e. any planned output file does not exist yet)\n",
    "if len(commands) > 0:\n",
    "    \n",
    "    ## Save commands to file\n",
    "    slurm_cmd_filepath = f\"{output_dir}/slurm.cmd\"\n",
    "    with open(slurm_cmd_filepath, \"w\") as f:\n",
    "        f.write(\"\\n\".join(commands).strip())\n",
    "    print(f\"New file created: {slurm_cmd_filepath}\")\n",
    "\n",
    "    ## Write SLURM script\n",
    "    slurm_arrays_n = len(commands)\n",
    "    slurm_job_id = random.randint(1, 999999)\n",
    "    slurm_script = f\"\"\"#!/bin/sh \n",
    "\n",
    "    #SBATCH --job-name=alphafold_array_job_{slurm_job_id}\n",
    "    #SBATCH --time=06:00:00\n",
    "    #SBATCH --output={slurm_logs_dir}/slurm%A_%a.out\n",
    "    #SBATCH --error={slurm_logs_dir}/slurm%A_%a.err\n",
    "    #SBATCH --array=1-{slurm_arrays_n}%{min(16,slurm_arrays_n)}\n",
    "    #SBATCH --mem=64G \n",
    "    #SBATCH --cpus-per-task=8 \n",
    "    #SBATCH --partition=a100 \n",
    "    #SBATCH --gres=gpu:1\n",
    "    #SBATCH --wait\n",
    "\n",
    "    module load AlphaFold/2.2.0\n",
    "\n",
    "    $(head -$SLURM_ARRAY_TASK_ID {slurm_cmd_filepath} | tail -1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove any leading spaces (very important, otherwise SLURM will fail!)\n",
    "    slurm_script = \"\\n\".join([s.lstrip() for s in slurm_script.split(\"\\n\")])\n",
    "\n",
    "    ## Save script to file\n",
    "    slurm_sh_filepath = f\"{output_dir}/slurm.sh\"\n",
    "    with open(slurm_sh_filepath, \"w\") as f:\n",
    "        f.write(slurm_script.strip())\n",
    "    print(f\"New file created: {slurm_sh_filepath}\")\n",
    "\n",
    "\n",
    "    # Execute SLURM batch job\n",
    "    print(f\"Launching SLURM batch script: {slurm_sh_filepath}\")\n",
    "    print(f\"Planned tasks: {len(commands)} total.\")\n",
    "\n",
    "    #returncode, job_id = 0, 12345678  # debugging\n",
    "    \n",
    "    # For debugging purposes, comment-out the following line to avoid launching the SLURM job when not required\n",
    "    returncode, job_id, tasks, completed_tasks, running_tasks, pending_tasks, error_tasks = execute_sbatch_script(slurm_sh_filepath, check_status=True, check_status_sleep=30)\n",
    "\n",
    "    if returncode == 0:\n",
    "        print(f\"SLURM job {job_id} completed.\")\n",
    "    else:\n",
    "        print(f\"SLURM batch job {job_id} encountered some problems. Exit code: {returncode}. Please read the SLURM documentation to learn more\")\n",
    "    \n",
    "    # Print SLURM summary\n",
    "    slurm_status = get_sjobexitmod(job_id)\n",
    "    print(\"Summary:\\n\\n\" + slurm_status)\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(\"All scheduled output files already exist!\")\n",
    "    print(f\"Manually remove files from '{output_dir}' if you wish to repeat structure prediction with AlphaFold.\")\n",
    "\n",
    "\n",
    "\n",
    "# Parse AlphaFold output files\n",
    "\n",
    "print(\"Parsing AlphaFold output files...\")\n",
    "\n",
    "for i, protein_id in enumerate(db.keys()):\n",
    "    # Get important files returned by AlphaFold\n",
    "    pdb_filepath = [o for o in db[protein_id]['predicted_structures']['output_file_paths'] if o.endswith(\"ranked_0.pdb\")]\n",
    "    pdb_filepath = pdb_filepath[0] if len(pdb_filepath) > 0 else \"\"\n",
    "\n",
    "    if os.path.isfile(pdb_filepath):\n",
    "\n",
    "        distance_matrix = calculate_distance_matrix(pdb_filepath)\n",
    "        \n",
    "        ####\n",
    "        \n",
    "        feature_dict_filepath = [o for o in db[protein_id]['predicted_structures']['output_file_paths'] if o.endswith(\"features.pkl\")]        \n",
    "        feature_dict_filepath = feature_dict_filepath[0] if len(feature_dict_filepath) > 0 else \"\"\n",
    "        \n",
    "        # last try\n",
    "        if feature_dict_filepath == \"\":\n",
    "            feature_dict_filepath = os.path.normpath(os.path.dirname(pdb_filepath)+\"/features.pkl\")\n",
    "            feature_dict_filepath = feature_dict_filepath if os.path.isfile(feature_dict_filepath) else \"\"\n",
    "\n",
    "        if os.path.isfile(feature_dict_filepath):\n",
    "\n",
    "            # Upack features pickle\n",
    "            with open(feature_dict_filepath, 'rb') as p:\n",
    "                feature_dict = pickle.load(p)\n",
    "                \n",
    "            if 'msa' in feature_dict.keys():\n",
    "                msa = feature_dict['msa']\n",
    "            else:\n",
    "                msa = None\n",
    "\n",
    "        else:\n",
    "            # File model_dict does not exist\n",
    "            print(f\"Pickle file missing: {feature_dict_filepath}. MSA will not be displayed!\")\n",
    "            # MSA and sequence coverage will be missing from db and report\n",
    "            msa = None\n",
    "\n",
    "        ####\n",
    "    \n",
    "        model_pkl_filepath = [o for o in db[protein_id]['predicted_structures']['output_file_paths'] if o.endswith(\"pred_0.pkl\")]\n",
    "        model_pkl_filepath = model_pkl_filepath[0] if len(model_pkl_filepath) > 0 else \"\"\n",
    "\n",
    "        # last try\n",
    "        if model_pkl_filepath == \"\":\n",
    "            model_pkl_filepath = os.path.normpath(os.path.dirname(pdb_filepath)+\"/result_model_1_ptm_pred_0.pkl\")\n",
    "            model_pkl_filepath = model_pkl_filepath if os.path.isfile(model_pkl_filepath) else \"\"\n",
    "\n",
    "        if model_pkl_filepath == \"\":\n",
    "            model_pkl_filepath = os.path.normpath(os.path.dirname(pdb_filepath)+\"/result_model_1_pred_0.pkl\")\n",
    "            model_pkl_filepath = model_pkl_filepath if os.path.isfile(model_pkl_filepath) else \"\"\n",
    "\n",
    "        \n",
    "        if os.path.isfile(model_pkl_filepath):\n",
    "            #  Unpack model pickle\n",
    "            with open(model_pkl_filepath, 'rb') as p:\n",
    "                model_dict = pickle.load(p)\n",
    "            \n",
    "             # Get PAE\n",
    "            if 'predicted_aligned_error' in model_dict.keys():\n",
    "                pae = model_dict['predicted_aligned_error']\n",
    "            else:\n",
    "                pae = None\n",
    "                     \n",
    "            # Get pLDDT\n",
    "            if 'plddt' in model_dict.keys():\n",
    "                plddt = model_dict['plddt']\n",
    "            else:\n",
    "                plddt = extract_b_factors(pdb_filepath)\n",
    "    \n",
    "            # Get \"overall score\"\n",
    "            if 'ranking_confidence' in model_dict.keys():\n",
    "                ranking_confidence = model_dict['ranking_confidence']\n",
    "            else:\n",
    "                if 'plddt' in model_dict.keys():\n",
    "                    # For AF monomer should be equal to model_dict['ranking_confidence']\n",
    "                    # -> https://github.com/deepmind/alphafold/issues/348#issuecomment-1384215171\n",
    "                    ranking_confidence = np.mean(plddt)\n",
    "                    \n",
    "        else:\n",
    "            # File model_dict does not exist\n",
    "            print(f\"Pickle file missing: {model_pkl_filepath}. PAE will not be displayed!\")\n",
    "            # pLDDT and ranking_confidence can still be extracted from the PDB file\n",
    "            # However, PAE will be missing from db and report\n",
    "            pae = None\n",
    "            plddt = extract_b_factors(pdb_filepath)\n",
    "            ranking_confidence = np.mean(plddt)\n",
    "\n",
    "        ####\n",
    "\n",
    "        # Plot stats\n",
    "        \n",
    "        combined_filepath = plot_combined_figures(msa, pae, np.clip(distance_matrix, 0, 15), plddt, \n",
    "                                                     output_filepath=pdb_filepath.replace(\".pdb\", \"_plots.svg\"))\n",
    "        if combined_filepath not in db[protein_id]['predicted_structures']['output_file_paths']:\n",
    "            db[protein_id]['predicted_structures']['output_file_paths'].append(combined_filepath)\n",
    "\n",
    "        # Put data in db\n",
    "        db[protein_id]['predicted_structures']['output_data']    = {\"msa\" : msa, \"caca\" : distance_matrix, \"pae\" : pae, \"plddt\" : plddt}\n",
    "        db[protein_id]['predicted_structures']['report_keywords'] = {}\n",
    "        db[protein_id]['predicted_structures']['report_score']    = ranking_confidence # TODO: make it proportional to the number of aligned sequences -> a model created from alignment with 1 sequence is less good than one with 100 sequences\n",
    "\n",
    "    else:\n",
    "        # The required AlphaFold output PDB file is missing\n",
    "        print(f\"Structure file missing: {pdb_filepath}\")\n",
    "\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22885e",
   "metadata": {},
   "source": [
    "### 2.6. Structure similarity search\n",
    "\n",
    "Each imported sequence is searched with Foldseek in each one of these databases:\n",
    "\n",
    "- PDB\n",
    "- AlphaFold Proteome\n",
    "- AlphaFold UniProt50\n",
    "\n",
    "<div style=\"background-color: #e1f0ff; padding: 12px; border-radius: 7px; box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.1);\">\n",
    "     <b>Information:</b> The following code block executes a resource-intensive process. If output files for a particular sequence and searched database already exist, the existing results will be preserved, and Foldseek will not be executed again for those sequences.  \n",
    "To perform fresh calculations on sequences that have already been processed, please manually remove the corresponding output files or directories before rerunning this code block.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75648ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which databases to use for Foldseek. Provide them as dict {name1 : path1, name2 : path2, ...}\n",
    "databases_to_search = {\"pdb\"            : \"/your/own/database/folder/foldseek/db/pdb_2023_08_18\",\n",
    "                       \"afdb-proteome\"  : \"/your/own/database/folder/foldseek/db/afdb_p\",\n",
    "                       \"afdb-uniprot50\" : \"/your/own/database/folder/foldseek/db/afdb_u50\"}\n",
    "\n",
    "## The Foldseek database can be obtained by running: foldseek databases PDB DESTINATION_FILE TMP_FOLDER\n",
    "## Other databases: https://github.com/steineggerlab/foldseek#databases\n",
    "## Custom database: https://github.com/steineggerlab/foldseek#create-custom-databases-and-indexes\n",
    "\n",
    "\n",
    "# Make output folders\n",
    "output_dir     = mdir(STRUCTURE_SIMILARITY_FOLDER, chmod=777)\n",
    "slurm_logs_dir = mdir(f\"{output_dir}/slurm_logs\")\n",
    "\n",
    "# Add items to db\n",
    "for protein_id in db.keys():\n",
    "    db[protein_id]['similar_structures']['databases']         = databases_to_search\n",
    "    db[protein_id]['similar_structures']['output_file_paths'] = []\n",
    "\n",
    "# Compile a SLURM script to run Foldseek for each protein sequence in the database\n",
    "## Write SLURM batch array commands\n",
    "commands = []\n",
    "for database_name, database_path in databases_to_search.items():\n",
    "    for protein_id in db.keys():\n",
    "        input_filepath = [o for o in db[protein_id]['predicted_structures']['output_file_paths'] if o.endswith(\"ranked_0.pdb\")]\n",
    "        input_filepath = input_filepath[0] if len(input_filepath) > 0 else \"\"\n",
    "        output_filepath = f\"{output_dir}/{protein_id}_{database_name}_foldseek.tsv\"\n",
    "    \n",
    "        if os.path.isfile(input_filepath):\n",
    "    \n",
    "            # Add expected output filepaths to db\n",
    "            db[protein_id]['similar_structures']['output_file_paths'].append(output_filepath)\n",
    "            \n",
    "            if os.path.isfile(output_filepath):\n",
    "                # If expected output file already exists, don't add it again to the commands file.\n",
    "                pass\n",
    "            else:\n",
    "                tmp_folder  = mdir(f\"{output_dir}/{protein_id}\", verbose=False)\n",
    "                input_filepath  = shutil.copy(input_filepath, f\"{tmp_folder}/{protein_id}.pdb\")\n",
    "        \n",
    "                # Write SLURM command\n",
    "                command = f\"foldseek easy-search {input_filepath} {database_path} {output_filepath} {tmp_folder} --format-output query,target,fident,alnlen,mismatch,gapopen,qstart,qend,tstart,tend,evalue,bits,prob,theader\"\n",
    "\n",
    "                # Add to list\n",
    "                commands.append(command)\n",
    "                print(f\"Structure added to the SLURM queue: {protein_id}, database: {database_name}\")\n",
    "        else:\n",
    "            print(f\"Structure missing: {protein_id} (Foldseek will not run for this protein)\")\n",
    "\n",
    "            ## For additional Foldseek output see: https://github.com/steineggerlab/foldseek#tab-separated\n",
    "        \n",
    "## Check if there are any new commands (i.e. any planned output file does not exist yet)\n",
    "if len(commands) > 0:\n",
    "    ## Save commands to file\n",
    "    slurm_cmd_filepath = f\"{output_dir}/slurm.cmd\"\n",
    "    with open(slurm_cmd_filepath, \"w\") as f:\n",
    "        f.write(\"\\n\".join(commands).strip())\n",
    "    print(\"New file created: %s\"%slurm_cmd_filepath)\n",
    "    \n",
    "    ## Write SLURM script\n",
    "    slurm_arrays_n = len(commands)\n",
    "    slurm_job_id = random.randint(1, 999999)\n",
    "    slurm_script = f\"\"\"#!/bin/sh\n",
    "    \n",
    "    #SBATCH --job-name=foldseek_array_job_{slurm_job_id}\n",
    "    #SBATCH --time=06:00:00\n",
    "    #SBATCH --output={slurm_logs_dir}/slurm%A_%a.out\n",
    "    #SBATCH --error={slurm_logs_dir}/slurm%A_%a.err\n",
    "    #SBATCH --array=1-{slurm_arrays_n}%{min(24,slurm_arrays_n)}\n",
    "    #SBATCH --mem=64G \n",
    "    #SBATCH --cpus-per-task=12 \n",
    "    #SBATCH --partition=scicore \n",
    "    #SBATCH --wait\n",
    "    \n",
    "    module load Foldseek\n",
    "    module load MMseqs2\n",
    "    \n",
    "    $(head -$SLURM_ARRAY_TASK_ID {slurm_cmd_filepath} | tail -1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Remove any leading spaces (very important, otherwise SLURM will fail!)\n",
    "    slurm_script = \"\\n\".join([s.lstrip() for s in slurm_script.split(\"\\n\")])\n",
    "    \n",
    "    ## Save script to file\n",
    "    slurm_sh_filepath = f\"{output_dir}/slurm.sh\"\n",
    "    with open(slurm_sh_filepath, \"w\") as f:\n",
    "        f.write(slurm_script.strip())\n",
    "    print(\"New file created: %s\"%slurm_sh_filepath)\n",
    "\n",
    "    \n",
    "    ## Launch Foldseek SLURM job\n",
    "    print(f\"Launching SLURM batch script: {slurm_sh_filepath}\")\n",
    "    print(f\"Planned tasks: {len(commands)} total.\")\n",
    "    \n",
    "    returncode, job_id, tasks, completed_tasks, running_tasks, pending_tasks, error_tasks = execute_sbatch_script(slurm_sh_filepath, check_status=True, check_status_sleep=15)\n",
    "    \n",
    "    if returncode == 0:\n",
    "        print(f\"SLURM job {job_id} completed.\")\n",
    "    else:\n",
    "        print(f\"SLURM batch job {job_id} encountered some problems. Exit code: {returncode}. Please read the SLURM documentation to learn more\")\n",
    "    \n",
    "    # Print SLURM summary\n",
    "    slurm_status = get_sjobexitmod(job_id)\n",
    "    print(\"Summary:\\n\\n\" + slurm_status)\n",
    "\n",
    "    # Remove temporary folders\n",
    "    for protein_id in db.keys():\n",
    "        dir = f\"{output_dir}/{protein_id}\"\n",
    "        if os.path.isdir(dir):\n",
    "            shutil.rmtree(dir)\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"All scheduled output files already exist!\")\n",
    "    print(f\"Manually remove files from '{output_dir}' if you wish to repeat structure similarity search with Foldseek.\")\n",
    "\n",
    "\n",
    "# Parse foldseek\n",
    "\n",
    "print(\"Parsing Foldseek output files...\")\n",
    "\n",
    "threshold_key   = list(THRESHOLDS['similar_structures'].keys())[0]\n",
    "threshold_value = THRESHOLDS['similar_structures'][threshold_key]\n",
    "print(f\"Applying {threshold_key} threshold at {threshold_value}...\")\n",
    "\n",
    "for protein_id in db.keys():\n",
    "    filepath_list = db[protein_id]['similar_structures']['output_file_paths']\n",
    "    db[protein_id]['similar_structures']['output_data'] = []\n",
    "\n",
    "    descriptions = []\n",
    "    stacks = []\n",
    "    \n",
    "    for filepath in filepath_list:\n",
    "        if filepath.endswith(\"foldseek.tsv\"):\n",
    "\n",
    "            file_type = filepath.split(\"_\")[-2]\n",
    "            parsed_blasttab = parse_blasttab(filepath, file_type)\n",
    "\n",
    "             # Score for report\n",
    "            if len(parsed_blasttab) > 0:\n",
    "                minval = np.min([p[\"evalue\"] for p in parsed_blasttab])\n",
    "                if db[protein_id]['similar_structures']['report_score'] == -1 or minval < db[protein_id]['similar_structures']['report_score']:\n",
    "                    db[protein_id]['similar_structures']['report_score'] = minval\n",
    "\n",
    "            # Apply threshold\n",
    "            if threshold_key == 'evalue':\n",
    "                parsed_blasttab = [p for p in parsed_blasttab if p[\"evalue\"] <= threshold_value]\n",
    "            db[protein_id]['similar_structures']['report_threshold'] = threshold_value\n",
    "\n",
    "            for pbt in parsed_blasttab:\n",
    "                descriptions.append(pbt['description'].replace(\"-\", \"_\"))\n",
    "            \n",
    "            db[protein_id]['similar_structures']['output_data'] += parsed_blasttab\n",
    "\n",
    "            # Plot the HHRs separately for each .hhr file\n",
    "            #stack = group_non_overlapping(parsed_blasttab)[:10]\n",
    "            #plot_filepath = plot_stack(stack, len(db[protein_id]['sequence']), filepath.replace(\".tsv\",\".svg\"))\n",
    "            \n",
    "            #if plot_filepath not in db[protein_id]['similar_structures']['output_file_paths']:\n",
    "            #    db[protein_id]['similar_structures']['output_file_paths'].append(plot_filepath)\n",
    "\n",
    "            # Stack the HHRs for a combined plot\n",
    "            stack = group_non_overlapping(parsed_blasttab) # Recalculate stack after threshold application\n",
    "            if len(stack) != 0:\n",
    "                stacks.append((filepath.split(\"_\")[-2], stack[:5]))\n",
    "\n",
    "    # Plot all in a single figure\n",
    "    combined_plot_filepath = plot_combined_stack(stacks, len(db[protein_id]['sequence']), \"_\".join(filepath.split(\"_\")[:-2])+\"_merged.svg\")\n",
    "    if combined_plot_filepath not in db[protein_id]['similar_structures']['output_file_paths']:\n",
    "        db[protein_id]['similar_structures']['output_file_paths'].append(combined_plot_filepath)\n",
    "    \n",
    "    db[protein_id]['similar_structures']['report_keywords'] = get_keywords(\" \".join(descriptions))\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e9599-d8a8-402a-a4f5-76f5e706dab3",
   "metadata": {},
   "source": [
    "## 3. Data export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49804b8",
   "metadata": {},
   "source": [
    "### 3.1. Generate HTML report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a7363",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Report template files\n",
    "INDEX_TEMPLATE_FILE = \"./report_template_files/report_index_template.html\"\n",
    "ENTRY_TEMPLATE_FILE = \"./report_template_files/report_entry_template.html\"\n",
    "\n",
    "if os.path.isfile(INDEX_TEMPLATE_FILE) and os.path.isfile(ENTRY_TEMPLATE_FILE):\n",
    "\n",
    "    print(\"Preparing the report...\")\n",
    "\n",
    "    # Create REPORT_FOLDER\n",
    "    output_dir = mdir(REPORT_FOLDER)\n",
    "    \n",
    "    def get_relative_path(filepath1, filepath2):\n",
    "        commonpath  = os.path.commonpath([filepath1, filepath2])\n",
    "        relpath     = filepath1.replace(commonpath, \"\")\n",
    "        for subfolder in range(filepath2.replace(commonpath, \"\").count(\"/\")-1):\n",
    "            relpath = \"../\" + relpath\n",
    "        if not relpath.startswith(\".\"):\n",
    "            relpath = f\"./{relpath}\"\n",
    "        return os.path.normpath(relpath)\n",
    "\n",
    "    ###########################\n",
    "    # Create Entry page\n",
    "    \n",
    "    for i, protein in enumerate(db.keys()):\n",
    "        current = i+1\n",
    "        entry_html_filepath = f\"{output_dir}/entries/{current}.html\"\n",
    "        \n",
    "        ## Load individual entry page template\n",
    "        with open(ENTRY_TEMPLATE_FILE, \"r\") as f:\n",
    "            entry_html = f.read()\n",
    "    \n",
    "        ### Navigation\n",
    "        if i > 0 and i < len(db.keys())-1:\n",
    "            entry_html = entry_html.replace(\"{{navigation}}\", f'<a href=\"../index.html\">Return to summary</a> | <a href=\"./{current-1}.html\">Go to previous</a> | <a href=\"./{current+1}.html\">Go to next</a>')\n",
    "        elif i == 0:\n",
    "            entry_html = entry_html.replace(\"{{navigation}}\", f'<a href=\"../index.html\">Return to summary</a> | <a href=\"./{current+1}.html\">Go to next</a>')\n",
    "        else:\n",
    "            entry_html = entry_html.replace(\"{{navigation}}\", f'<a href=\"../index.html\">Return to summary</a> | <a href=\"./{current-1}.html\">Go to previous</a>')\n",
    "\n",
    "\n",
    "        #\n",
    "        entry_id = protein\n",
    "        entry_name = db[protein]['name']\n",
    "        entry_sequence = db[protein]['sequence']\n",
    "        entry_filepath = get_relative_path(db[protein]['sequence_file_path'], entry_html_filepath)\n",
    "        entry_molecular_weight = \"%.2f\"%ProteinAnalysis(db[protein]['sequence']).molecular_weight()\n",
    "        entry_imported_annotations = db[protein]['imported_annotations']\n",
    "            \n",
    "        ### Title and file details\n",
    "        entry_html = entry_html.replace(\"{{entry_name}}\",    entry_name)\n",
    "        entry_html = entry_html.replace(\"{{datetime}}\",      ctime())\n",
    "        entry_html = entry_html.replace(\"{{projectfolder}}\", PROJECT_FOLDER)\n",
    "        entry_html = entry_html.replace(\"{{inputfile}}\",     INPUT_FILE)\n",
    "    \n",
    "        \n",
    "        ### Sequence information\n",
    "        entry_html = entry_html.replace(\"{{entry_id}}\", entry_id)\n",
    "        entry_html = entry_html.replace(\"{{entry_imported_annotations}}\", entry_imported_annotations if len(entry_imported_annotations.strip()) > 0 else \"--\")\n",
    "        entry_html = entry_html.replace(\"{{entry_sequence}}\", \"<br>\".join([entry_sequence[i:i+60] for i in range(0, len(entry_sequence), 60)]))\n",
    "        entry_html = entry_html.replace(\"{{entry_sequence_residue_n}}\", str(len(entry_sequence)))\n",
    "        entry_html = entry_html.replace(\"{{entry_sequence_molecular_weight}}\", entry_molecular_weight)\n",
    "        entry_html = entry_html.replace(\"{{entry_fasta_filepath}}\", f\"<a href='{entry_filepath}'>{entry_filepath}</a>\")\n",
    "    \n",
    "        ### Domain architecture\n",
    "        domain_architecture_databases = [d.capitalize() for d in db[protein]['domain_architecture']['databases'].keys()]\n",
    "        domain_architecture_file_paths = db[protein]['domain_architecture']['output_file_paths']\n",
    "        domain_architecture_relpaths = [get_relative_path(p, entry_html_filepath) for p in domain_architecture_file_paths]\n",
    "        domain_architecture_relpaths.sort()\n",
    "        domain_architecture_plots_paths = [p for p in domain_architecture_file_paths if p.endswith('.svg')]\n",
    "        domain_architecture_top_keywords = list(db[protein]['domain_architecture']['report_keywords'].keys())[:10]\n",
    "        domain_architecture_top_keywords_html = f\"<b>{', '.join(domain_architecture_top_keywords)}</b>\" if len(domain_architecture_top_keywords) > 0 else \"--\"\n",
    "\n",
    "        headers = ['db', 'id', 'prob', 'evalue', 'pvalue', 'score', 'cols', 'query', 'query_len', 'template', 'template_len', 'name', 'description']\n",
    "        headers_html = \"<tr>\"\n",
    "        for i, h in enumerate(headers):\n",
    "            headers_html += f\"<th>{h}</th>\" if i < len(headers)-1 else f\"<th style='width=100%'>{h}</th>\"\n",
    "        headers_html += \"</tr>\"\n",
    "        rows_html = \"\"\n",
    "        for d in db[protein]['domain_architecture']['output_data']:\n",
    "            rows_html += \"<tr>\"\n",
    "            for i, h in enumerate(headers):\n",
    "                rows_html += f\"<td>{d[h]}</td>\" if i < len(headers)-1 else f\"<td style='width=100%'>{d[h]}</td>\"\n",
    "            rows_html += \"</tr>\"\n",
    "        \n",
    "        domain_architecture_table_html = f'<div class=\"scrollable_area\"><table border=\"1\" cellspacing=\"0\" cellpadding=\"5\" width=\"100%\"><thead>{headers_html}</thead><tbody>{rows_html}</tbody></table></div>' if len(db[protein]['domain_architecture']['output_data']) > 0 else \"--\"\n",
    "    \n",
    "        entry_html = entry_html.replace(\"{{domain_architecture_databases}}\",\n",
    "                                        \", \".join(domain_architecture_databases))\n",
    "        entry_html = entry_html.replace(\"{{domain_architecture_plots}}\",\n",
    "                                        \"<br>\".join([open(p).read() for p in domain_architecture_plots_paths]))\n",
    "        entry_html = entry_html.replace(\"{{domain_architecture_table}}\",\n",
    "                                        domain_architecture_table_html)\n",
    "        entry_html = entry_html.replace(\"{{domain_architecture_keywords}}\",\n",
    "                                        domain_architecture_top_keywords_html)\n",
    "        entry_html = entry_html.replace(\"{{domain_architecture_filepaths}}\",\n",
    "                                        \"<br>\".join([f\"<a href='{p}'>{p}</a>\" for p in domain_architecture_relpaths]))\n",
    "        entry_html = entry_html.replace(\"{{domain_architecture_threshold}}\", \"%.2e (%s)\"%(list(THRESHOLDS['domain_architecture'].values())[0], list(THRESHOLDS['domain_architecture'].keys())[0]))\n",
    "    \n",
    "        \n",
    "        \n",
    "        ### Identical proteins in sequence and structure databases\n",
    "        identical_proteins_databases = [d.capitalize() for d in db[protein]['identical_sequences']['databases'].keys()]\n",
    "        identical_proteins_file_paths = db[protein]['identical_sequences']['output_file_paths']\n",
    "        identical_proteins_relpaths = [get_relative_path(p, entry_html_filepath) for p in identical_proteins_file_paths]\n",
    "        identical_proteins_relpaths.sort()\n",
    "        identical_proteins_relpaths_html = \"<br>\".join([f\"<a href='{p}'>{p}</a>\" for p in identical_proteins_relpaths]) if len(identical_proteins_relpaths) > 0 else \"--\"\n",
    "        \n",
    "        identical_proteins_found_entries_html = \"\"\n",
    "        for db_name, accessions, descriptions, organisms in db[protein]['identical_sequences']['output_data']:\n",
    "            sublist = \"\"\n",
    "            for i, a in enumerate(accessions):\n",
    "                d = descriptions[i].strip()\n",
    "                o = f\"({organisms[i].strip()})\"\n",
    "                sublist += f\"<li>{a}: {d} {o}</li>\"\n",
    "            identical_proteins_found_entries_html += f\"{db_name.capitalize()}<ul>{sublist}</ul>\"\n",
    "        identical_proteins_found_entries_html = \"--\" if identical_proteins_found_entries_html == \"\" else identical_proteins_found_entries_html\n",
    "        \n",
    "        identical_proteins_top_keywords = list(db[protein]['identical_sequences']['report_keywords'].keys())[:10]\n",
    "        identical_proteins_top_keywords_html = f\"<b>{', '.join(identical_proteins_top_keywords)}</b>\" if len(identical_proteins_top_keywords) > 0 else \"--\"\n",
    "    \n",
    "        entry_html = entry_html.replace(\"{{identical_proteins_databases}}\",\n",
    "                                        \", \".join(identical_proteins_databases))\n",
    "        entry_html = entry_html.replace(\"{{identical_proteins_table}}\",\n",
    "                                        identical_proteins_found_entries_html)\n",
    "        entry_html = entry_html.replace(\"{{identical_proteins_keywords}}\", \n",
    "                                        identical_proteins_top_keywords_html)\n",
    "        entry_html = entry_html.replace(\"{{identical_proteins_filepaths}}\", \n",
    "                                        identical_proteins_relpaths_html)\n",
    "        \n",
    "        \n",
    "        ### Sequence similarity search results (HHblits)\n",
    "        similar_sequences_databases = [d.capitalize() for d in db[protein]['similar_sequences']['databases'].keys()]\n",
    "        similar_sequences_file_paths = db[protein]['similar_sequences']['output_file_paths']\n",
    "        similar_sequences_relpaths = [get_relative_path(p, entry_html_filepath) for p in similar_sequences_file_paths]\n",
    "        similar_sequences_relpaths.sort()\n",
    "        similar_sequences_plots_paths = [p for p in similar_sequences_file_paths if p.endswith('merged.svg')]\n",
    "        similar_sequences_top_keywords = list(db[protein]['similar_sequences']['report_keywords'].keys())[:10]\n",
    "        similar_sequences_top_keywords_html = f\"<b>{', '.join(similar_sequences_top_keywords)}</b>\" if len(similar_sequences_top_keywords) > 0 else \"--\"\n",
    "\n",
    "        headers = ['db', 'id', 'prob', 'evalue', 'pvalue', 'score', 'cols', 'query', 'query_len', 'template', 'template_len', 'name', 'description']\n",
    "        headers_html = \"<tr>\"\n",
    "        for i, h in enumerate(headers):\n",
    "            headers_html += f\"<th>{h}</th>\" if i < len(headers)-1 else f\"<th style='width=100%'>{h}</th>\"\n",
    "        headers_html += \"</tr>\"\n",
    "        rows_html = \"\"\n",
    "        for d in db[protein]['similar_sequences']['output_data']:\n",
    "            rows_html += \"<tr>\"\n",
    "            for i, h in enumerate(headers):\n",
    "                rows_html += f\"<td>{d[h]}</td>\" if i < len(headers)-1 else f\"<td style='width=100%'>{d[h]}</td>\"\n",
    "            rows_html += \"</tr>\"\n",
    "        \n",
    "        similar_sequences_table_html = f'<div class=\"scrollable_area\"><table border=\"1\" cellspacing=\"0\" cellpadding=\"5\" width=\"100%\"><thead>{headers_html}</thead><tbody>{rows_html}</tbody></table></div>' if len(db[protein]['similar_sequences']['output_data']) > 0 else \"--\"\n",
    "        \n",
    "        entry_html = entry_html.replace(\"{{similar_sequences_databases}}\",\n",
    "                                        \", \".join(similar_sequences_databases))\n",
    "        entry_html = entry_html.replace(\"{{similar_sequences_plots}}\",\n",
    "                                        \"<br>\".join([open(p).read() for p in similar_sequences_plots_paths]))\n",
    "        entry_html = entry_html.replace(\"{{similar_sequences_table}}\",\n",
    "                                        similar_sequences_table_html)\n",
    "        entry_html = entry_html.replace(\"{{similar_sequences_keywords}}\",\n",
    "                                        similar_sequences_top_keywords_html)\n",
    "        entry_html = entry_html.replace(\"{{similar_sequences_filepaths}}\",\n",
    "                                        \"<br>\".join([f\"<a href='{p}'>{p}</a>\" for p in similar_sequences_relpaths]))\n",
    "        entry_html = entry_html.replace(\"{{similar_sequences_threshold}}\", \"%.2e (%s)\"%(list(THRESHOLDS['similar_sequences'].values())[0], list(THRESHOLDS['similar_sequences'].keys())[0]))\n",
    "    \n",
    "        \n",
    "        ### Structure prediction results (AlphaFold)\n",
    "        predicted_structure_file_paths = db[protein]['predicted_structures']['output_file_paths']\n",
    "        predicted_structure_pdb_filepath = [p for p in predicted_structure_file_paths if p.endswith('ranked_0.pdb')]\n",
    "        predicted_structure_pdb_filepath = predicted_structure_pdb_filepath[0] if len(predicted_structure_pdb_filepath) > 0 else \"\"\n",
    "        predicted_structure_pdb = \"\"\n",
    "        if os.path.isfile(predicted_structure_pdb_filepath):\n",
    "            with open(predicted_structure_pdb_filepath, \"r\") as f:\n",
    "                predicted_structure_pdb = f.read()\n",
    "        \n",
    "        predicted_structure_relpaths = [get_relative_path(p, entry_html_filepath) for p in predicted_structure_file_paths]\n",
    "        predicted_structure_relpaths.sort()\n",
    "        predicted_structure_confidence_plot_paths = [p for p in predicted_structure_file_paths if p.endswith('_plots.svg')]\n",
    "        predicted_structure_other_plot_paths = [p for p in predicted_structure_file_paths if (p.endswith('.svg') and p not in predicted_structure_confidence_plot_paths)]\n",
    "    \n",
    "        entry_html = entry_html.replace(\"{{predicted_structure_plots}}\",\n",
    "                                        \"<br>\".join([open(p).read() for p in predicted_structure_confidence_plot_paths]) if len(predicted_structure_confidence_plot_paths) > 0 else \"<b>Files missing!</b>\")\n",
    "        \n",
    "        entry_html = entry_html.replace(\"{{predicted_structure_pdb}}\",\n",
    "                                        predicted_structure_pdb)\n",
    "    \n",
    "        entry_html = entry_html.replace(\"{{predicted_structure_filepaths}}\", #f\"<a href='{predicted_structure_pdb_relpath}'>{predicted_structure_pdb_relpath}</a>\")\n",
    "                                        \"<br>\".join([f\"<a href='{p}'>{p}</a>\" for p in predicted_structure_relpaths]))\n",
    "\n",
    "        \n",
    "        ### Structure similarity search results (Foldseek)\n",
    "        similar_structures_databases  = [d.capitalize() for d in db[protein]['similar_structures']['databases'].keys()]\n",
    "        similar_structures_file_paths = db[protein]['similar_structures']['output_file_paths']\n",
    "        similar_structures_relpaths   = [get_relative_path(p, entry_html_filepath) for p in similar_structures_file_paths]\n",
    "        similar_structures_relpaths.sort()\n",
    "        similar_structures_plots_paths = [p for p in similar_structures_file_paths if p.endswith('.svg')]\n",
    "\n",
    "        similar_structures_top_keywords = list(db[protein]['similar_structures']['report_keywords'].keys())[:10]\n",
    "        similar_structures_top_keywords_html = f\"<b>{', '.join(similar_structures_top_keywords)}</b>\" if len(similar_structures_top_keywords) > 0 else \"--\"\n",
    "\n",
    "        #headers = ['db', 'id', 'prob', 'evalue', 'pvalue', 'score', 'cols', 'query', 'query_len', 'template', 'template_len', 'name', 'description']\n",
    "        headers = ['db', 'id', 'prob', 'evalue', 'bits', 'fident', 'alnlen', 'mismatch', 'gapopen', 'qstart', 'qend', 'tstart', 'tend', 'name', 'description']\n",
    "        headers_html = \"<tr>\"\n",
    "        for i, h in enumerate(headers):\n",
    "            headers_html += f\"<th>{h}</th>\" if i < len(headers)-1 else f\"<th style='width=100%'>{h}</th>\"\n",
    "        headers_html += \"</tr>\"\n",
    "        rows_html = \"\"\n",
    "        for d in db[protein]['similar_structures']['output_data']:\n",
    "            rows_html += \"<tr>\"\n",
    "            for i, h in enumerate(headers):\n",
    "                rows_html += f\"<td>{d[h]}</td>\" if i < len(headers)-1 else f\"<td style='width=100%'>{d[h]}</td>\"\n",
    "            rows_html += \"</tr>\"\n",
    "        \n",
    "        similar_structures_table_html = f'<div class=\"scrollable_area\"><table border=\"1\" cellspacing=\"0\" cellpadding=\"5\" width=\"100%\"><thead>{headers_html}</thead><tbody>{rows_html}</tbody></table></div>' if len(db[protein]['similar_structures']['output_data']) > 0 else \"--\"\n",
    "\n",
    "        \n",
    "        entry_html = entry_html.replace(\"{{similar_structures_databases}}\",\n",
    "                                        \", \".join(similar_structures_databases))\n",
    "        entry_html = entry_html.replace(\"{{similar_structures_plots}}\",\n",
    "                                        \"<br>\".join([open(p).read() for p in similar_structures_plots_paths]))\n",
    "        entry_html = entry_html.replace(\"{{similar_structures_results_table}}\",\n",
    "                                        similar_structures_table_html)\n",
    "        entry_html = entry_html.replace(\"{{similar_structures_keywords}}\",\n",
    "                                        similar_structures_top_keywords_html)\n",
    "        entry_html = entry_html.replace(\"{{similar_structures_filepaths}}\",\n",
    "                                        \"<br>\".join([f\"<a href='{p}'>{p}</a>\" for p in similar_structures_relpaths]))\n",
    "        entry_html = entry_html.replace(\"{{similar_structures_threshold}}\", \"%.2e (%s)\"%(list(THRESHOLDS['similar_structures'].values())[0], list(THRESHOLDS['similar_structures'].keys())[0]))\n",
    "\n",
    "\n",
    "        ## Wordcloud\n",
    "        keywords_dict_list = [db[protein]['domain_architecture']['report_keywords'],\n",
    "                              db[protein]['identical_sequences']['report_keywords'],\n",
    "                              db[protein]['similar_sequences']['report_keywords'],\n",
    "                              db[protein]['predicted_structures']['report_keywords'],\n",
    "                              db[protein]['similar_structures']['report_keywords']]\n",
    "\n",
    "        kw_dict = combine_dicts(keywords_dict_list)\n",
    "        if len(kw_dict) > 0:\n",
    "            tmp_file = f\"{entry_html_filepath}.wc.svg\"\n",
    "            entry_wordcloud = WordCloud(background_color=\"white\",\n",
    "                                        width=500,\n",
    "                                        height=150,\n",
    "                                        max_words=50,\n",
    "                                        prefer_horizontal=0.7,\n",
    "                                        min_font_size=6).generate_from_frequencies(kw_dict).to_svg(tmp_file)\n",
    "\n",
    "            if os.path.isfile(tmp_file):\n",
    "                os.remove(tmp_file)\n",
    "\n",
    "        else:\n",
    "            entry_wordcloud = \"\"\n",
    "\n",
    "        entry_html = entry_html.replace(\"{{wordcloud}}\", entry_wordcloud)\n",
    "        \n",
    "        \n",
    "        ## Save file\n",
    "        mdir(f\"{output_dir}/entries\")\n",
    "        with open(entry_html_filepath, \"w\") as f:\n",
    "            f.write(entry_html)\n",
    "\n",
    "\n",
    "    ###########################\n",
    "    # Create Index page\n",
    "    \n",
    "    index_table_rows = []\n",
    "    for i, protein in enumerate(db.keys()):\n",
    "        current = i+1\n",
    "        entry_name = db[protein]['name']\n",
    "        entry_name = entry_name[:37]+\"...\" if len(entry_name) > 40 else entry_name\n",
    "\n",
    "        entry_a = db[protein]['domain_architecture']['report_score']\n",
    "        entry_b = db[protein]['identical_sequences']['report_score']\n",
    "        entry_c = db[protein]['similar_sequences']['report_score']\n",
    "        entry_d = db[protein]['predicted_structures']['report_score']\n",
    "        entry_e = db[protein]['similar_structures']['report_score']\n",
    "\n",
    "        keywords_dict_list = [db[protein]['domain_architecture']['report_keywords'],\n",
    "                              db[protein]['identical_sequences']['report_keywords'],\n",
    "                              db[protein]['similar_sequences']['report_keywords'],\n",
    "                              db[protein]['predicted_structures']['report_keywords'],\n",
    "                              db[protein]['similar_structures']['report_keywords']]\n",
    "\n",
    "        kw_dict = combine_dicts(keywords_dict_list)\n",
    "        entry_keywords = list(kw_dict.keys())[:20]\n",
    "\n",
    "        entry_keywords_html = []\n",
    "        for kw in kw_dict.keys():\n",
    "            kw_key = kw\n",
    "            kw_freq = kw_dict[kw]\n",
    "            if kw_freq >= 2:\n",
    "                entry_keywords_html.append(f\"{kw}\")\n",
    "            else:\n",
    "                entry_keywords_html.append(f'<span style=\"color:#8D8D8D;\">{kw}</span>')\n",
    "        entry_keywords_html = \", \".join(entry_keywords_html[:20])\n",
    "        \n",
    "     \n",
    "        cell_style_a = \"bgcolor='#d6fad4' style='color:black;'\" if entry_a < db[protein]['domain_architecture']['report_threshold'] and entry_a >= 0 else \"style='color:grey;'\"\n",
    "        cell_style_b = \"bgcolor='#d6fad4' style='color:black;'\" if entry_b > 0 else \"style='color:grey;'\"\n",
    "        cell_style_c = \"bgcolor='#d6fad4' style='color:black;'\" if entry_c < db[protein]['similar_sequences']['report_threshold'] and entry_c >= 0 else \"style='color:grey;'\"\n",
    "        cell_style_d = \"bgcolor='#d6fad4' style='color:black;'\" if entry_d > 70 else \"style='color:grey;'\"  \n",
    "        cell_style_e = \"bgcolor='#d6fad4' style='color:black;'\" if entry_e < db[protein]['similar_structures']['report_threshold'] and entry_e >= 0 else \"style='color:grey;'\" \n",
    "                \n",
    "        index_row = f\"\"\"<tr>\n",
    "                            <td>{i+1}</td>\n",
    "                            <td><a href=\"./entries/{current}.html\">{entry_name}</a>\n",
    "                            <div class=\"content-single\" style=\"display: none;\">{\"Locus: \"+db[protein]['locus']+\"<br>\" if db[protein]['locus'] != \"\" else \"\"}Seq: {db[protein]['sequence'][:18]}...<br>Seq Len: {db[protein]['sequence_length']} aa<br>MW: {'%.1f'%(float(db[protein]['sequence_mw'])/1000)} kDa</div></td>\n",
    "                            <td {cell_style_a}>{\"%.1e\"%entry_a if entry_a != -1 else \"--\"}</td>\n",
    "                            <td {cell_style_b}>{entry_b}</td>\n",
    "                            <td {cell_style_c}>{\"%.1e\"%entry_c if entry_c != -1 else \"--\"}</td>\n",
    "                            <td {cell_style_d}>{\"%.1f\"%entry_d if entry_d != -1 else \"--\"}</td>\n",
    "                            <td {cell_style_e}>{\"%.1e\"%entry_e if entry_e != -1 else \"--\"}</td>\n",
    "                            <td>\n",
    "                                <div class=\"content-all\">\n",
    "                                {entry_keywords_html}\n",
    "                                </div>\n",
    "                                \n",
    "                                <div class=\"content-single\" style=\"display: none;\">\n",
    "                                <div class=\"sub-table\">\n",
    "                                <table>\n",
    "                                <tr><td><b>DA </b></td><td>{\", \".join(list(db[protein]['domain_architecture']['report_keywords'].keys())[:20])}</td></tr>\n",
    "                                <tr><td><b>IP </b></td><td>{\", \".join(list(db[protein]['identical_sequences']['report_keywords'].keys())[:20])}</td></tr>\n",
    "                                <tr><td><b>SeS</b></td><td>{\", \".join(list(db[protein]['similar_sequences']['report_keywords'].keys())[:20])}</td></tr>\n",
    "                                <tr><td><b>StS</b></td><td>{\", \".join(list(db[protein]['similar_structures']['report_keywords'].keys())[:20])}</td></tr>\n",
    "                                </table>\n",
    "                                </div>\n",
    "                                </div>\n",
    "                            </td>\n",
    "                        </tr>\n",
    "                     \"\"\"\n",
    "\n",
    "        \n",
    "        index_table_rows.append(index_row)\n",
    "    \n",
    "    \n",
    "    ## Load report index (summary) page template\n",
    "    with open(INDEX_TEMPLATE_FILE, \"r\") as f:\n",
    "        index_html = f.read()\n",
    "    \n",
    "    ## Replace index page content\n",
    "    index_html = index_html.replace(\"{{report_name}}\",   os.path.basename(INPUT_FILE))\n",
    "    index_html = index_html.replace(\"{{datetime}}\",      ctime())\n",
    "    index_html = index_html.replace(\"{{projectfolder}}\", PROJECT_FOLDER)\n",
    "    index_html = index_html.replace(\"{{inputfile}}\",     INPUT_FILE)\n",
    "    index_html = index_html.replace(\"{{table_rows}}\",    \"\\n\".join(index_table_rows))\n",
    "    index_html = index_html.replace(\"{{domain_architecture_threshold}}\", \"%.2e\"%list(THRESHOLDS['domain_architecture'].values())[0])\n",
    "    index_html = index_html.replace(\"{{sequence_similarity_threshold}}\", \"%.2e\"%list(THRESHOLDS['similar_sequences'].values())[0])\n",
    "    index_html = index_html.replace(\"{{structure_similarity_threshold}}\", \"%.2e\"%list(THRESHOLDS['similar_structures'].values())[0])\n",
    "    \n",
    "    ## Save summary page\n",
    "    report_filepath = f\"{output_dir}/index.html\"\n",
    "    with open(report_filepath, \"w\") as f:\n",
    "        f.write(index_html)\n",
    "    \n",
    "    print(f\"Report generated: {report_filepath}\")\n",
    "    \n",
    "else:\n",
    "    # Report template files missing\n",
    "    print(f\"Report template files are missing. Expected filepaths: {INDEX_TEMPLATE_FILE}, {ENTRY_TEMPLATE_FILE}.\")\n",
    "    print(\"Report cannot be generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ac3ca-f866-40db-98ce-8a98f89a4c43",
   "metadata": {},
   "source": [
    "### 3.2. Zip HTML report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c57e7-a63b-4b1d-b299-5b8350b84de9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Zip report\n",
    "import shutil\n",
    "shutil.make_archive(f\"{PROJECT_FOLDER}/report\", 'zip', output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416ce24-42e1-472c-a802-24d9f86bd76a",
   "metadata": {},
   "source": [
    "### 3.3. Export local database to JSON file (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c4389-7ab4-416b-a094-73c541662188",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# JSON export\n",
    "import json\n",
    "\n",
    "def sort_db(db): # optional\n",
    "    for protein_id in db.keys():\n",
    "        db[protein_id] = dict(sorted(db[protein_id].items()))\n",
    "\n",
    "with open(f\"{PROJECT_FOLDER}/data.json\", \"w\") as f:\n",
    "    f.write(json.dumps(db))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-eps_pipeline_env]",
   "language": "python",
   "name": "conda-env-miniconda3-eps_pipeline_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
